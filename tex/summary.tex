\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{color}
\usepackage{comment}

\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand\abs[1]{\left|#1\right|}

\title{Unsupervised Learning in Neural Networks\\\Huge \textsc{Lecture Notes}}
\author{Contributors: Fabian Brix, Renato Kempter}

\begin{document}
\maketitle

\section{Neuroscience}

\subsection{Visual Cortex}

\subsubsection{Primary Visual Cortex}
The primary visual cortex is the simplest, earliest cortical visual area. It is highly specialized for processing information about static and moving objects and is excellent in pattern recognition.

\subsection{Receptive Fields}
The term receptive field originally described an area of the body service where a stimulus could elicit a reflex (Sherrington, 1906). The definition was later extended to sensory neurons defining the receptive field as a restricted region of visual space where a luminous stimulus could drive electrical responses in a retinal ganglion cell: \textit{"Responses can be obtained in a given optic nerve fiber only upon illumination of a certain restricted region of the retina, termed the receptive field of the fiber"} (Hartline, 1938).
$http://www.scholarpedia.org/article/Receptive_field$
\paragraph*{General definition} spanning different types of neurons across sensory modalities: the receptive field is a portion of sensory space that can elicit neuronal responses when stimulated. The sensory space can be defined in a single dimension (e.g. carbon chain length of an odorant), two dimensions (e.g. skin surface) or multiple dimensions (e.g. space, time and tuning properties of a visual receptive field). The neuronal response can be defined as \textsc{firing rate} (i.e. \textbf{number of action potentials generated by a neuron}) or include also subthreshold activity (i.e. depolarizations and hyperpolarizations in membrane potential that do not generate action potentials).

\subsubsection{Visual Receptive Fields}
ICA 2, discourse receptive fields. 

\paragraph{Receptive Field Development}


\section{Supervised vs. Unsupervised learning}
\paragraph{Supervised learning} a system is trained for regression or classification via a set of labeled training data. The goal is to optimize the parameters of an objective function using the labeled data (How to correctly predict the labelling "l" or "f" of input patterns according to the training data?). Supervised learning is dealt with in Prof. Seegers class "Pattern Classification and Machine Learning".
\paragraph{Unsupervised learning} In unsupervised learning one tries to extract patterns from the data without having any information about their classification/outputs. One therefore tries to detect the intrinsic structure of the data. (How does an hand-written "l" or "f" look like?). 
\paragraph{Reinforcement learning}
In this context an "agent" is employed and given a certain choice of actions and rewards upon chosing the correct action. The actions exist according to training input patterns (animal conditioning: a mouse sees an "l" or an "f" and chooses action $a_1$/$a_2$ accordingly and is given a reward upon success).
Reinforcement learning can be either conducted in a supervised or an unsupervised manner.

\section{Hebbian Learning Rule}
change in a given synaptic weight is proportional to both the pre-synaptic input and the output activity of the post-synaptic neuron.
\begin{itemize}
\item time-dependent
\item local
\item strongly interactive
\end{itemize}
\subsection{Synaptic plasticity}
%source: http://icwww.epfl.ch/~gerstner/SPNM/node71.html
change in connection strength
\begin{quote}{Hebb, 1949:}
\textit{"When an \textcolor{red}{axon} of cell \textcolor{red}{j} repeatedly or persistently takes part in firing (through \textcolor{blue}{synapses} to \textcolor{green}{dendrites} of) cell \textcolor{green}{i}, then \textcolor{red}{j's} efficiency as one of the cells firing is increased"}.
\end{quote}
The Hebbian learning rule is a local \textsc{unsupervised learning} rule, because it depends solely on the states of the respective pre-synaptic i and post-synaptic j neurons and the present efficacy $w_{ij}$, but not on the state of other neurons k. The repsective pre-synaptic neurons and post-synaptic neuron can however be simultaneously active, so that correlations in their activity lead to firing of the neuronal cell \textcolor{red}{i}. In Hebbian learning these correlations are used to learn the synaptic transmission efficacies $w_{ij}$, so that in essence it is correlation-based learning.

\subsection{Homeostatic plasticity}
Homeostatic plasticity refers to the capacity of neurons to regulate their own excitability relative to network activity, a compensatory adjustment that occurs over the timescale of days. The term homeostatic plasticity derives from two opposing concepts: "homeostatic" (a product of the Greek words for "same" and "state" or "condition") and plasticity (or "change"), thus homeostatic plasticity means "staying the same through change."

\subsection{Rate-based Hebbian Learning}
\paragraph{Goal} is to find a mathematically formulated learning rule based on Hebb's postulate.\\
We begin by focusing on a single synapse with efficacy $w_{ij}$ transmitting signals from a pre-synaptic neuron i to a post-synaptic neuron j. The activity ((mean) firing rate - average number of spikes per unit time) of i is denoted by $v_i$ and that of j by $v_j$. Based on the (1) \texttt{locality} aspect of Hebbian Learning we can write a general description of the change of the synaptic efficacy. 
\[
\frac{d}{dt}w_{ij}=\mathbf{F}(w_{ij}; v_j^{pre}, v_i^{post})
\]
The membrane potential is further uniquely defined by the postsynaptic firing rate $v_i = g(u_i)$ with a monotone gain function g and therefore doesn't have to be included in F.\\
The (2) \texttt{cooperativity} aspect of Hebb's postulates implies that pre- and postsynaptic neuron have to be active simultaneously for a synaptic weight change to occur and this property can be used to learn something about the function F. Taylor series expansion around $v_i=v_j=0$ leads to:
\[
\frac{d}{dt}w_{ij}=c_0(w_{ij})+c_1^{post}(w_{ij})v_i+c_1^{pre}(w_ij)v_j+c_2^{pre}(w_{ij})v_j^2+c_2^{post}(w_{ij})v_i^2+c_2^{corr}v_i v_j+\mathcal{O}(v^3)
\]
The term with $c_2^{corr}$ is bilinear in pre- and postsynaptic activity showing that the change of the synaptic efficacy $w_{ij}$ is indeed subject to a combination of changes in both activities.\\
Simplest choice of F is to fix $c_2^{corr}=c>0$ and set all other terms of the taylor expansion to zero resulting in the prototype of hebbian learning.\\
\[
\frac{d}{dt}w_{ij}=c_2^{corr}v_i v_j
\]
This learning rule includes only first-order terms and therefore models non-Hebbian plasticity. More complicated learning rules include higher-order terms of the taylor expansion. If $c_2^{corr}=c<0$, the learning rule is anti-hebbian.\\
F is dependet on the efficacy $w_{ij}$, so that it will not grow without limit. A saturation of synaptic weights can be achieved if parameter $c_2^{corr}$ goes to zero as $w_{ij}$ approaches its maximum value:
\[
	c_2^{corr}(w_{ij})=\gamma_2(1-w_{ij})
\]
Originally Hebb did not consider a rule for decreasing the synaptic weights. In order to have a feasible model however, the synaptic efficacy should decrease in the absence of stimulation.\\
\[
	c_0(w_{ij})=-\gamma_0 w_{ij}
\]
The combination results in the simplest feasible learning rule:
\[
\frac{d}{dt}w_{ij}=\gamma_2(1-w_{ij})v_i v_j-\gamma_0 w_{ij}
\]
\subsubsection{Gating}
\paragraph{Postsynaptic gating} A weight change occurs only if the postsynaptic neuron is active, $v_i>0$. $\Rightarrow$ the weight changes are "gated" by the postsynaptic neuron. Only the efficacies of highly active presynaptic neurons $v_j>v_{\phi}$ are strengthened.
\[
\frac{d}{dt}w_{ij}=\gamma v_i(v_j-v_{\phi}(w_{ij})), \gamma>0
\]
\paragraph{Presynaptic gating} role of pre- and postsynaptic firing rate are exchanged. Now, a change in synaptic weights can only occur if the presynaptic neuron is active, $v_j>0$ and the activity of the postsynaptic neuron determines the direction of the change.
\[
\frac{d}{dt}w_{ij}=\gamma v_j(v_i-v_{\phi}))
\]

\subsubsection{BCM rule}
The "Bienenstock-Cooper-Munroe" rule is a generalization of the presynaptic gating rule, where $\Phi$ is a non-linear function and the reference rate $v_\phi$ is the running average of the postsynaptic activity $v_i$. 
\[
\frac{d}{dt}w_{ij}=\eta \Phi(v_i-v_{\phi}))v_j-\gamma w_{ij}
\]
%For low values of the postsynaptic activity (y<θM), ϕ is negative; for y>θM , ϕ is positive. The rule stabilizes by allowing the modification threshold, θM , to vary as a super-linear function of the previous activity of the cell. Unlike traditional methods of stabilizing Hebbian learning, this "sliding threshold" provides a mechanism for incoming patterns, as opposed to converging afferents, to compete.
%In order to avoid that the postsynaptic firing rate blows up or decays to zero, it is therefore necessary to turn  $ \nu_{\theta}^{}$ into an adaptive variable.
\subsection{Learning in Rate Models (functional consequences of hebbian learning)}
\paragraph{Goal:} understand how activity-dependent learning rules influence the formation of connections between neurons in the brain.\\
Plasticity is controlled by the statistical properties of the presynaptic input for the postsynaptic neuron.
\subsubsection{Evolution of synaptic weights} For the sake of simplicity we model the presynaptic input as a set of static patterns $\{x^\mu \in \mathcal{R}^N; 0\leq \mu \leq p\}$. At each time step one of the patterns is selected at random and the presynaptic rates are fixed to $v_i = x_i^\mu$. The synaptic weights are modified according to a Hebbian learning rule dependent on the correlation of pre- and postsynaptic activity: $\frac{d}{dt}w_{ij}=a_2^{corr}v_i^{post}v_j^{pre}$.\\
In a general rate model the firing rate $v_i^{post}$ is given by a nonlinear function of the total input: $v^{post}=g(\sum_i w_i v_i^{pre})$. For simplification reasons we focus on a linear rate model: 
\[
v_i^{post}=\sum_k w_{ik} v_k^{pre}
\]
We can now combine the learning rule and the linear rate model to form:
\[
\frac{d}{dt}w_{ij}=a_2^{corr} \sum_k w_{ik} v_k^{pre} v_j^{pre}
\]
We are interested in the long-term behaviour of the synaptic weights and therefore consider the expectation value of the weight vector, i.e., the weight vector averaged over the sequence $(x^{\mu_1}, x^{\mu_2}, ..., x^{\mu_n})$ that have so far been presented to the network.
\begin{align*}
&\langle \frac{d}{dt}w_{ij} \rangle = a_2^{corr}\sum_k \langle w_{ik} \rangle \langle \underbrace{x_k^\mu x_j^\mu}_{C_{kj}} \rangle
&C_{kj}=\langle x_kx_j \rangle = \frac{1}{p}\sum_{\mu=1}^p x_k^\mu x_j^\mu
\end{align*}
$C_{kj}$ are the correlation matrices for respective components i and j in the sum of the learning rule. Using PCA we can detect the direction of maximal variance in the set of patterns. $\Rightarrow$ Express weight vector $w$ in eigenvectors of C. \textsc{What do principal components mean in this respect?}
\paragraph{PCA} think of neuronal output as projection of weight vector, fill in PCA equations
\subsection{Oja's rule}
The Oja learning rule is a mathematical formalization of the Hebbian learning rule, such that over time the neuron actually learns to compute a principal component of its input stream. The forgetting term added to balance the growth of the weights this time is not only proportional to the value of the weight, but also to the square of the output of the neuron $v_i$.
\[
\frac{d}{dt}w_{ij}=a_2^{corr}v_jv_i-\underbrace{a_2^{post}}_{a_2^{corr}w_{ij}}(v_i^{post})^2=a_2^{corr}(v_jv_i-w_{ij}(v_i^{post})^2)
\]
Now, the forgetting term balances the growth of the weight. The squared output $(v_i^{post})^2$ guarantees that the larger the output of the neuron becomes, the stronger is this balancing effect.
\subsubsection{Oja's rule and PCA}
\[
v_i^{post}=\mathbf{w}^T\mathbf{x}^\mu
\]
\[
\frac{d}{dt}\mathbf{w}=a_2^{corr}(\mathbf{x}^\mu \mathbf{x}^{\mu T}\mathbf{w}-\mathbf{w}^T\mathbf{x}^\mu \mathbf{x}^{\mu T}\mathbf{w}\mathbf{w}), \{x^\mu, 0 \leq \mu \leq p\}
\]
This is the incremental change for just one input vector $x^\mu$. When the algorithm is run for a long time, changing the input vector at every step, one can look at the average behaviour. An especially interesting question is what is the value of the weights when the average change in the weight is zero. This is the point of convergence of the algorithm. Averaging right hand side over $x^\mu$ while $w$ stays constant yields the following equation.
\[
\frac{d}{dt}\mathbf{w}=a_2^{corr}(\underbrace{\langle \mathbf{x}\mathbf{x^T} \rangle}_{C} \mathbf{w}-\underbrace{(\mathbf{w}^T\underbrace{\langle \mathbf{x}\mathbf{x^T} \rangle}_{C} \mathbf{w}}_{\lambda})\mathbf{w})=0, \overline{x}=0
\]
Considering that the quadratic form $w^TCw$ is a scalar, this equation clearly is the eigenvalue-eigenvector equation for the covariance matrix C. This analysis shows that if the weights converge in the Oja learning rule $C\mathbf{w}=\lambda\mathbf{w}$, then the weight vector becomes one of the eigenvectors of the input covariance matrix $\mathbf{w}=\mathbf{e_1}$ (\textsc{only this state is stable}), and the output of the neuron becomes the corresponding principal component. Principal components are defined as the inner products between the eigenvectors and the input vectors. For this reason, the simple neuron learning by the Oja rule becomes a principal component analyzer (PCA).
Although not shown here, it has been proven that the neuron will find the the \texttt{first principal component} and the norm of the weight vector tends to one.
\textsc{PCA is used for Hebbian learning in \textbf{linear} neurons}

\subsection{Independent Component Analysis (ICA)}

\paragraph{Goal:} Compute a transformation such that the entries of the transformed vector (components) are mutually independent. Statistical Independence stronger than uncorrelatedness required by the PCA. (Two conditions are equivalent \textit{only} for Gaussian random variables). 

\paragraph{Assumption:} Random input vector $\mathbf{x}$ is generated by a linear combination of statistically independent $p(x_1,\dots,x_n)=p_1(x_1)\dots p_n(x_n)$ and stationary components (sources), $\mathbf{x}=A\mathbf{y}$. The task is now is to find a matrix W so as to recover the components of $\mathbf{y}$, the original signal sources $\mathbf{y}=\begin{bmatrix}s_1 & s_2 & \dots & s_n\end{bmatrix}^T$ by exploiting the information hidden in many samples of the mixed signal patterns $\mathbf{x}$ (blind source separation). $\mathcal{A}$ is known as the mixing and $W$ as the demixing matrix, respectively.\\
In contrast to PCA which can always be performed, ICA is meaningful only if the involved random variables are non-Gaussian. Each of the resulting independent components is \texttt{uniquely} estimated up to a multiplicative constant. For this reason the components are considered to be of unit variance.
Searching for independent rather than uncorrelated features gives us the means of exploiting a lot more information, hidden in the higher order statistics of the data.
\paragraph{Preprocessing (whitening)} \begin{itemize}
\item Zero mean data
\item Whitening (unit variance). Perform PCA on mixed signals and divide each component by the square-root of its eigenvalue $x_k^n=\frac{1}{\sqrt{\lambda_k}}x_k \Rightarrow C^n=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}$. After PCA entries of projections $\mathbf{y}$ of random vectors are uncorrelated and independence can be rewritten to that of projections $p(y_1,\dots y_n)=p_1(y_1)\dots p_n(y_n)$
\end{itemize}
How to find natural axis? We need measure for non-gaussianty.
If we look at two-dimensional data, an arbitrary projection will be close to a Gaussian distribution which doesn't tell us anything about our data (neuronal inputs). $\Rightarrow$ search for direction which is maximally non-Gaussian.\\
\subsubsection{Gaussianity vs. non-gaussianity}
This approach in performing ICA is a direct generalization of the PCA technique. The Karhunen-Loève transform focuses on the second-order statistics and demands the cross-correlations $\langle y_iy_j \rangle$ to be zero. In ICA demanding that the components of $\mathbf{y}$ be statistically independent is equivalent to demanding all the higher order cross-cumulants to be zero (going up to the fourth-order cumulants is sufficiant for many applications).%A Gaussian pdf is the least interesting pdf. The Gaussian is the most "random" of all pdfs
% source: Pattern Recognition, Chapter 6, Theodoridis, Koutroumbas
careful: Cross-cumulant vs auto-cumulant
cumulants vs. moments? These are actually moments?
\begin{align*}
&\mathcal{K}_1(y(i))=\E[y(i)]=0,\quad \text{\textbf{mean}}\\
&\mathcal{K}_2(y(i)y(j))=\E[y(i)y(j)],\quad \text{\textbf{(Co)variance}}\\
&\mathcal{K}_3(y(i)y(j)y(k))=\E[y(i)y(j)y(k)],\quad \text{\textbf{skewness}}
\end{align*}
\begin{align*}
\mathcal{K}_4(y(i)y(j)y(k)y(r))&=\E[y(i)y(j)y(k)y(r)]-\E[y(i)y(j)]\E[y(k)y(r)]\\
&-\E[y(i)y(k)]\E[y(j)y(r)]-\E[y(i)y(r)]\E[y(j)y(k)]\\
&=\E[y^4]-3\E[y^2],\quad \text{\textbf{kurtosis}}
\end{align*}
We need a measure for non-gaussianity, which we find in the higher order cumulants, because for Gaussian distributions there is no difference in ICA \& PCA. The kurtosis (4-th order cumulant) is such a measure of non-gaussianity. The smaller it is the more it describes a leptokurtic curve with large values around the mean descending fast towards the first standard deviation. The problem can now be reduced to finding a matrix, W, for that the second-order (covariacnes) and fourth-order cross-cumulants, kurtosis, of the transformed patterns are zero.\\
\textsc{Does anyone understand the meaning of the following equation?}
\[
J(w)=[\E(F(y))-\E(F(v))]^2
\]
\subsubsection{FastICA}
\subsubsection{Temporal ICA}
\paragraph{Goal:} Find unmixing matrix W such that outputs of time-dependent signals are independent. 
\paragraph{Assumptions:} Signals are time-dependendent and the different components may be recorded with delays $\tau_i$
\[
	\langle y_i(t)y_k(t-\tau_k) \rangle = \delta_{ik}\lambda(\tau)
\]
\section{Competitive Learning}

\subsection{Clustering}
\subsubsection{Nearest Neighbour}
Initialize prototypes $p_k$ to represent groups of data points. Assign datapoints to nearest neighbouring prototype $\abs{\mathbf{p}_k-\mathbf{x}}\leq\abs{\mathbf{p}_i-\mathbf{x}}$ resulting in Voronoi tesselation of input space. For data compression the input $\mathbf{x}$ is classified and the index k transmitted. This gives rise to the construction error
\[
\E(\mathbf{w}_1, \dots, \mathbf{w}_n) = \sum_k \sum_{\mu \in C_k} (\mathbf{w}_k-\mathbf{x}^\mu)^2
\]
But how to choose initialization of prototypes? To minimize the reconstruction error one has discretize the input space adaptively.
\subsubsection{K-Means}
Standard K-means is based on a random initialization of prototypes. 
\begin{enumerate}
\item For every data pattern $x^\mu$ the winning prototype $\mathbf{p}_k$is determined with the nearest neighbour rule.
\item The winning prototype is moved closer to the classified $\mathbf{x}_k^\mu$ by updating it according to $\Delta \mathbf{p}_k = \eta (\mathbf{p}_k-\mathbf{x}^\mu$).
\end{enumerate}
\paragraph{Showing that the error decreases}
Gradient descent on the error surface in the following manner 
\[
\Delta \mathbf{p}_j = -\eta \frac{\partial\E}{\partial\mathbf{w}_j} = -\eta \frac{\partial}{\partial\mathbf{w}_j} \sum_k \sum_{\mu \in C_k} (\mathbf{w}_k-\mathbf{x}^\mu)^2
\]
\begin{align*}
&\text{\textbf{batch update}}: \quad\Delta \mathbf{p}_j
= -2\eta \sum_{\mu \in C_j}(\mathbf{x^\mu}-\mathbf{w}_j)\\
&\text{\textbf{online update}}: \quad\Delta \mathbf{p}_j
= 2\eta (\mathbf{x^\mu}-\mathbf{w}_j)
\end{align*}
In the batch update rule distances from the data points in the prototype's class are summed up and the prototype is moved towards the mean with step size $\eta$.\\
After convergence: $\Delta w_k=0$
\begin{align*}
&\sum_{\mu \in C_k}\mathbf{w}_k-\sum_{\mu \in C_k}\mathbf{x}^\mu=N_k\mathbf{w}_k-\sum_{\mu \in C_k}\mathbf{x}^\mu\\
&\Rightarrow \mathbf{w}_k=\frac{1}{N_k}\sum_{\mu \in C_k}\mathbf{x}^\mu
\end{align*}
So with the batch update rule each prototype is at the center of its data cloud after convergence.\\
The online update has a stochastic component, because one has to choose a data point at random and move into its direction by the step size $\eta$. This causes the algorithm to jitter around the mean. To alleviate this problem the trick is to reduce $\eta$ so that the algorithm eventually freezes.
\paragraph{Problem of dead units}
Dead units are prototypes without data clouds. The error surface allows the algorithm to converge into a state permitting dead units through several local minima. To overcome this problem it is useful to initialize the prototypes to data points in k-means.

\subsection{Neuronal Implementation of Clustering}
(now multiple postsynaptic neurons for the same input?)
In the neuronal implementation the data patterns take the form of sensory inputs to neurons in the brain, one postsynaptic neuron for every prototype $\mathbf{p}_j$. The activity of these neurons is defined by a non-linear neuron-model $v_1^{post}=y_1=g(\sum_k \mathbf{w}_{ik}\mathbf{x}_k^\mu)=g(\mathbf{w}^T\mathbf{x}^\mu)$ which performs a non-linear transform of the scalar product of the prototypes used as weights $\mathbf{w}_j=\mathbf{w}_k$ which are now synaptic weights and the sensory inputs.\\
For \textsc{normalized weight vectors} the clustering expression $\abs{\mathbf{p}_k-\mathbf{x}}\leq\abs{\mathbf{p}_i-\mathbf{x}}$ can be rewritten to $\mathbf{w}_k^T\mathbf{x}\leq\mathbf{w}_k\mathbf{x}$. This means that the smallest distance to the prototype $\mathbf{w}_k$ results in the largest scalar product. We reformulate our clustering aim to finding the postsynaptic neuron with the maximal scalar product ("biggest total stimulation").

\paragraph{Winner take all circuit}
How can it be assured that only one neuron wins, that is to say that the respective input pattern is classified to a single prototype? Simply by introducing a \texttt{lateral term} incorporating activities of the other postsynaptic neurons for the other prototypes. The winner is the neuron that receives the most stimulation from outside (strongest outside drive) inhibiting the others which is equivalent to saying input pattern is classified to the prototype with which it forms the largest scalar product $\mathbf{w}_k\leq\mathbf{x}^\mu$.
\begin{align*}
&y_i(t+1)=g(\text{total drive})=g(\sum_k w(t+1)_{ik}\mathbf{x}_k^\mu+\sum_j B_{ij}y(t)_j),\quad B_{ij}<0\\
&\Rightarrow y_i(t^{final})=\left\{\begin{array}{1} \text{1 for winner}\\\text{0 for others}\end{array} \right
\end{align*}
How to update weights?

\paragraph{Hebbian Learning Rule (no inhibition)}
For every pattern $\mathbf{x}^\mu$ find winning prototype $\mathbf{w}_k=\mathbf{p}_k$ and update it according to the following learning rule:
\begin{align*}
\frac{d}{dt}w_{ki}&=\eta(y_k x_i^\mu-\gamma w_{ki}y_k)\\
&=\eta(\delta_{ki} x_i-w_{ki}\delta_{ki})\\
&=\eta(x_i-w_{ki})%\\\text{Why can y_k be regarded constant?}
\end{align*}
update for winner k (only neuron active) is same eq. as before (k-means), now derived as a hebbian learning rule:
\[
\frac{d}{dt} \mathbf{w}_{k}=\eta(\mathbf{x}^\mu-\mathbf{w}_{k})
\]

\paragraph{Kohonen Learning Rule}
\begin{enumerate}
\item choose input $\mathbf{x}^\mu$
\item determine winner k according to current weights for input (scalar product)
\item update weights of winner k \[
\frac{d}{dt} w_{ki}=\eta(y_kx_i^\mu-\gamma w_{ki}y_k)
\]
and neighbours j
\[
\frac{d}{dt} = \eta \Lambda(j,k)(\mathbf{x}^\mu-\mathbf{w}_j),\quad \text{on-line}
\] Every neighbour j is inhibited according to inhibition coefficients of k for j.
\item decrease size of neighbourhood function $\Lambda(j,k)$ to ensure convergence?
\item reiterate from 1) until stabilization
\end{enumerate}
Thus, the neuron whose weight vector was closest to the input vector is updated to be even closer. The result is that the winning neuron is more likely to win the competition the next time a similar vector is presented, and less likely to win when a very different input vector is presented. As more and more inputs are presented, each neuron in the layer closest to a group of input vectors soon adjusts its weight vector toward those input vectors. Eventually, if there are enough neurons, every cluster of similar input vectors will have a neuron that outputs 1 when a vector in the cluster is presented, while outputting a 0 at all other times. Thus, the competitive network learns to categorize the input vectors it sees.

\section{Reinforcement learning}

\paragraph{RL vs. supervised learning}
Supervised learning, used in statistical pattern recognition and artifical neural networks is learning from examples provided by a knowledgable external supervisor whereas in RL an agent is instructed to learn from its own experience. It is a ''behavioral'' learning problem: Through interaction between the learning system and its environment, whereas the system seeks to achieve a specific goal.

\paragraph{trade-off exploration vs. exploitation}
An agent has to exploit what it \textsc{already knows} in order to obtain a reward, but it also has to \textsc{explore} in order to make \textsc{better action selections} in the future. The agent must try a variety of actions and progressively favour those that appear to be best, because neither schemes can be pursued exclusively without failing a the task. Estimate reward probabilities vs. take action to maximize reward.

\paragraph{RLs main characteristic}
RL explicitly considers the \textsc{whole} problem of a goal-directed agent \textsc{interacting} with an uncertain environment. All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments. Moreover, it is usually assumed from the beginning that the agent has to operate despite significant \textsc{uncertainty about the environment} it faces. When reinforcement learning involves \textsc{planning}, it has to address the \textit{interplay between planning and real-time action selection}, as well as the question of how environmental models are acquired and improved.\\
An agent's actions are permitted to affect the future state of the environment (e.g., the next chess position, the level of reservoirs of the refinery, the next location of the robot), thereby affecting the options and opportunities available to the agent at later times. Correct choice requires taking into account indirect, delayed consequences of actions, and thus may require foresight or planning. The knowledge the agent brings to the task at the start--either from previous experience with related tasks or built into it by design or evolution--influences what is useful or easy to learn, but interaction with the environment is essential for adjusting behavior to exploit specific features of the task.

\subsection{Elements of reinforcement learning}
there are four main subelements of a reinforcement learning system.

\subsubsection*{Policy}
commonly denoted by $\pi$, it defines the learning agents way of behaving at a given time. Mapping from the perceived states of the environment to the actions that present themselves to the agent in these states. The policy alone is sufficient to determine the behaviour of an agent. In oder words, a policy is a rule used by the learning system to decide what to do, given the current state of the environment.

\subsubsection*{Reward function}
defines the goal in a reinforcement learning problem. Maps each perceived state of the environment to a single number, a reward, which describes the intrinsic desirability of the state. The agent's sole objective is to maximize its total gain from the rewards it receives.\\
The reward function may serve as a basis for altering the agent's policy.

\subsubsection*{Value function}
While the reward function indicates which actions are immediately beneficial to the agent, a \textbf{value function} specifies what is good for the agent in the long run. The value of a state is the \textsc{expected} total amount of rewards the agent will accumulate from this state onwards.
For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.

\subsubsection*{Model of the environment}
A model mimics the behaviour of the environment. \textit{For example, given a state and action, the model might predict the resultant next state and next reward}. Models are used for \textsc{planning}, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. The incorporation of models and planning into reinforcement learning systems is a relatively \textsc{new development}. Early reinforcement learning systems were explicitly trial-and-error learners; what they did was viewed as almost the opposite of planning. Nevertheless, it gradually became clear that reinforcement learning methods are closely related to dynamic programming methods, which do use models, and that they in turn are closely related to state-space planning methods. 

\subsection{Evolutionary methods}
Genetic algorithms, genetic programming, simulated annealing, and other function optimization methods have also been used to solve reinforcement learning problems. These methods search directly in the space of policies without ever appealing to value functions. We call these \textbf{evolutionary methods} because their operation is analogous to the way biological evolution produces organisms with skilled behaviour even when they do not learn during their individual lifetimes (organisms that have chosen adaptive policies survive). Effective when space of policies is sufficiently small or accurately sensing the state of the environment is impossible.

\subsection{Reward-based action learning: Q-values}

$Q(S,a)$ is the expected reward and can be written as $\sum_{s'} P_{s \overrightarrow s'}^a R_{s \overrightarrow s'}^a$. The optimal strategy is to take the action such that $Q(s,a^*) > Q(s, a_j)$. Q values are not known at the beginning, therefore, we need to learn them -> iteratively update. $\Delta Q(s,a) = \theta [r-Q(s,a)]$. If the mean update has converged, then $<\Delta Q(s,a)> = 0$

There are different strategies:
\begin{itemize}
	\item Greedy strategy
	\item $\eta$ - greedy strategy $P = 1 . \eta$
	\item Softmax strategy $P(a') = \frac{exp{[\beta Q(a')]}}{\sum_a exp{[\beta Q(a)]}}$
	\item Optimistic greedy (Q values too big at start)
\end{itemize}

\subsection{Bellmann Equation and Sarsa}
Multi step horizon reward-based action learning!
\[
	\Delta Q(s,a) = \eta [r-(Q(s,a) - \gamma Q(s', a'))]
\]
We can use the Bellman equation to write $Q(s,a)$
\[
	Q(s,a) = \sum_{s'}P_{s \rightarrow s'}^a * [R_{s \rightarrow s'} + \gamma \sum_{a'} \pi(s',a')Q(s',a')]
\]
The Bellman equation can be used for the SARSA algorithm:
\begin{enumerate}
	\item Being in state s, chose action a according to policy
	\item Observe reward r, next state s'
	\item Choose action a' in state s' according to policy
	\item Update $\Delta Q(s,a)$
	\item s' -> s; a' -> a
	\item Goto 1)
\end{enumerate}
We can rewrite the Bellman equation and express it as State values V.
\[
	V^{\pi}(s) = \sum_a \pi(s,a) \sum_{s'} P^a_{s \overrightarrow s'} [R^a_{s \overrightarrow s'} + \gamma * V^{\pi}(s')]
\]

which we can write in form of the value function as:

\[
	V^{\pi}(s) = \sum_a \pi(s,a) \sum_{s'} P_{s \rightarrow s'}^a[R_{s \rightarrow s'}^a + \gamma V^{\pi}(s')]
\]

The difference between $Q$ and $V$ is the following: The value function is the expected return starting from state $s_t$ and following policy $\pi$. The state-action value function $Q(s,a)$ is the expected return starting from state $s$, taking action $a$ and thereafter following policy $\pi$.

\subsection{On-policy vs off-policy learning}
On-policy means SARSA; off-policy means Q-learning. The difference lies in the method how to compute $Q(s,a)$. While for SARSA, we use the policy for the discounted term $\gamma \sum_{a'} \pi(s',a')Q(s',a')$, the Q-learning algo simply does $\gamma * max_{a'}Q(s',a')$. Therefore, Q-learning uses greedy for update and the policy for action selection ($\eta$ -- greedy)

\subsection{Eligibility traces}
At the moment of the reward update, update also previous action values along trajectories. The problem is that learning is very slow and the information diffusion across several states takes time.

$\gamma$ is the memory reduction parameter.
Every time an action $a$ is taken, one should do the following:
\begin{itemize}
	\item If a = action taken in state j: $e_{aj}(t) = \gamma * \lambda * e(s,a) + 1$
	\item else: $e(s,a)(t) = \gamma \lambda e(s,a)$ 
\end{itemize}
Then, for all states s and actions a:
\[
	\Delta Q(s,a) = \eta \delta_t e(s,a)
\]
An eligibility trace is a temporary record of the occurrence of an event, such as the visiting of a state or the taking of an action (backward view). The trace marks the memory parameters associated with the event as eligible for undergoing learning changes. The more theoretical view is that they are a bridge from TD to Monte Carlo methods.

\subsection{Continuous states}
% needs some more explanation, I didn't fully get it.
\[
	Q(s,a) = \sum_j w_{aj}*r_j(s) = \sum_j w_{aj}*\phi(s-s_j)
\]

If the time interval between subsequent weight updates approaches zero, the difference between two consecutive states $s' - s = v(a) * \Delta t$, where $v(a)$ is the velocity resulting from the choice of the action a. $s'-s$ approximates zero, therefore $s' \approx s$. Similarly, the finer the time resolution becomes, the likelier is that $a \approx a'$. Thus, for small time steps, an online gradient descent on the erro term $E_t$ becomes closer to a $\delta_t$ modulated Hebbian rule. 

\subsection{Temporal Difference TD($\lambda$)}
TD incorporates eligibility and continuous state space. We do a function approximation using weights and instead of updating $Q(s,a)$. In general, we do not work with $Q(s,a)$ but rather with the $V(s)$ values.

\[
	Q_w(s,a) = \sum_{i=1}^n w_i^a * r_i(s)
\]
\[
	\Delta w_{aj} = \eta \delta_t e_{aj}
\]

TODO: DOES SOMEBODY HAVE THE BLACKBOARD: REPRESENTATION OF STATE-VALUE Lecture 7 (Backgammon)

We can write the eligibility trace as $e_{aj}(t) = \gamma \lambda e_{aj}(t-\Delta t) + r_j\delta_{a,a'}$

\section{Energy-Based Models (EBM)}
A scalar energy is associated to each configuration of the variables of interest in the model (cost function). Learning then corresponds to modifying the energy function so that its shape has desirable properties, for example we would like desirable configurations to have low energy. The probabilistic distribution of for the model is define by an energy function:
\[
	p(x)=\frac{\exp(-E(x))}{Z}
\]
This distribution is the Boltzmann or Gibbs distribution which originates from classical statistical mechanics.\\
The normalization factor $Z$ is called the partition function $Z=\sum_x \exp(-E(x))$.\\
An energy-based model can be learnt by performing (stochastic) gradient descent on the empirical negative log-likelihood of the training data.

\subsection{EBMs with hidden units}
Often one does not observe the examples $x$ fully or one wants to introduce some non-observed variables to increase the expressive power of the model. So we consider a hidden part $y$ additionally to the observed part $x$, giving:
\[ 
	P(x) = \sum_y P(x,y) = \sum_y \frac{\exp(-E(x,y))}{Z}
\]
By introducing the notation of \textsc{free energy}
\[
	\mathcal{F}= -\log\sum_y \exp(-\mathcal{F})
\]
we can the rewrite the probability distribution in the condensed of the gibbs distribution:
\[
	P(x)=\frac{exp(-\mathcal{F}(x))}{Z}, with Z=\sum_x \exp(-\mathcal{F}(x))
\]

\subsection{Restricted Boltzmann Machines (RBM)}
Boltzmann Machines (BMs) are a particular form of log-linear (Gibbs distribution) Markov Random Fields (MRF), where the energy function is linear in its free parameters. In order to represent complicated distributions (limited parametric setting $\rightarrow$ non-parametric setting) we consider some of the variables to be hidden (not observed). One can increase the modeling capacity of the Boltzmann Machine (BM) by adding for hidden variables/units. Furthermore a RBM is restricted to a bipartite Graph, because there no lateral connections visible-visible and hidden-hidden are allowed. We introduce the notation of $v=x$ for visible units and $h=y$ for hidden units.\\
The energy function therefore is defined as:
\[
	E(v,h)=-b^{t}v-c^{t}h-h^{t}W_{u,v}u
\]
where $W_{u,v}$ represents a connection matrix of weights for hidden and visible units and $b,c$ are the offsets of the visible and hidden layers respectively.\\
By plugging in the energy function we get the following representation of the free energy formula:\\
\[
	F(v)=-b^{t}v-\sum_i\log\sum_{h_i} \exp(h_i(c_i+W_iv))
\]

Because of the specific structure of RBMs, visible and hidden units are conditionally independent of one-another 

A particular set of ''synaptic'' weights is said to constitute a perfect model of the environmental structure if it leads to exactly the same probability distribution of the states of the visible units as when these units are clamped by the environmental input vectors.

\subsubsection*{RBMs with binary units}
Commonly RBMs are studied with binary units $v_i, h_i \in \{0,1\}$ ...
The free energy simplifies to:\\
\[
	\mathcal{F}(v)=-b^{t}v-\sum_i\log(1+\exp(c_i+W_iv))
\]

deeplearning.net/tutorial/rbm.html

sigmoid function (sigm):
\[
	\sigma(t)=sig(t)=\frac{1}{1+\exp(-t)}=\frac{1}{2}\cdot(1+\tanh(\frac{t}{2}))
\]


\end{document}