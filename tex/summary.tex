\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{color}
\usepackage{comment}
\usepackage{listings}

\DeclareMathOperator{\E}{\mathbb{E}}
\newcommand\abs[1]{\left|#1\right|}
\newcommand{\vectornorm}[1]{\left\|#1\right\|}

\title{Unsupervised Learning in Neural Networks\\\Huge \textsc{Lecture Notes}}
\author{Contributors: Fabian Brix, Renato Kempter, Stig Viaene}

\begin{document}
\maketitle
\tableofcontents

\section{Neuroscience}

\subsection{Visual Cortex}

\subsubsection{Primary Visual Cortex}
The primary visual cortex is the simplest, earliest cortical visual area. It is highly specialized for processing information about static and moving objects and is excellent in pattern recognition.

\subsection{Receptive Fields}
The term receptive field originally described an area of the body service where a stimulus could elicit a reflex (Sherrington, 1906). The definition was later extended to sensory neurons defining the receptive field as a restricted region of visual space where a luminous stimulus could drive electrical responses in a retinal ganglion cell: \textit{"Responses can be obtained in a given optic nerve fiber only upon illumination of a certain restricted region of the retina, termed the receptive field of the fiber"} (Hartline, 1938).
$http://www.scholarpedia.org/article/Receptive\_field$
\paragraph*{General definition} spanning different types of neurons across sensory modalities: the receptive field is a portion of sensory space that can elicit neuronal responses when stimulated. The sensory space can be defined in a single dimension (e.g. carbon chain length of an odorant), two dimensions (e.g. skin surface) or multiple dimensions (e.g. space, time and tuning properties of a visual receptive field). The neuronal response can be defined as \textsc{firing rate} (i.e. \textbf{number of action potentials generated by a neuron}) or include also subthreshold activity (i.e. depolarizations and hyperpolarizations in membrane potential that do not generate action potentials).

\subsubsection{Visual Receptive Fields}
ICA 2, discourse receptive fields. 

\paragraph{Receptive Field Development}


\section{Supervised vs. Unsupervised learning}
\paragraph{Supervised learning} A system is trained for regression or classification via a set of labeled training data. The goal is to optimize the parameters of an objective function using the labeled data (How to correctly predict the labelling "l" or "f" of input patterns according to the training data?). Supervised learning is not considered in this class because there is no easy biological analogy. It is dealt with in Prof. Seegers class "Pattern Classification and Machine Learning".
\paragraph{Unsupervised learning} In unsupervised learning one tries to extract patterns from the data without having any information about their classification/outputs. One therefore tries to detect the intrinsic structure of the data. (How does an hand-written "l" or "f" look like?). 
\paragraph{Reinforcement learning}
In this context an "agent" is employed and given a certain choice of actions and rewards upon choosing the correct action. The actions exist according to training input patterns (animal conditioning: a mouse sees an "l" or an "f" and chooses action $a_1$/$a_2$ accordingly and is given a reward upon success).
Reinforcement learning can be either conducted in a supervised or an unsupervised manner.

\section{Hebbian Learning Rule}
change in a given synaptic weight is proportional to both the pre-synaptic input and the output activity of the post-synaptic neuron.
\begin{itemize}
\item time-dependent
\item local
\item strongly interactive
\end{itemize}
\subsection{Synaptic plasticity}
%source: http://icwww.epfl.ch/~gerstner/SPNM/node71.html
Synaptic plasticity is basically the change in connection strength of the synapses over time. According to the Hebbian learning rule, its changes are governed by the following postulate:
\begin{quote}{Hebb, 1949:}
\textit{"When an \textcolor{red}{axon} of cell \textcolor{red}{j} repeatedly or persistently takes part in firing (through \textcolor{blue}{synapses} to \textcolor{green}{dendrites} of) cell \textcolor{green}{i}, then \textcolor{red}{j's} efficiency as one of the cells firing is increased"}.
\end{quote}
The Hebbian learning rule is a local \textsc{unsupervised learning} rule, because it depends solely on the states of the respective pre-synaptic i and post-synaptic j neurons and the present efficacy $w_{ij}$, but not on the state of other neurons k. The respective pre-synaptic neurons and post-synaptic neuron can however be simultaneously active, so that correlations in their activity lead to firing of the neuronal cell \textcolor{red}{i}. In Hebbian learning these correlations are used to learn the synaptic transmission efficacies $w_{ij}$, so that in essence it is correlation-based learning. (A rigorous derivation is presented in Section \ref{sec:correlationDerivation}.)

\subsection{Homeostatic plasticity}
Homeostatic plasticity refers to the capacity of neurons to regulate their own excitability relative to network activity, a compensatory adjustment that occurs over the timescale of days. The term homeostatic plasticity derives from two opposing concepts: "homeostatic" (a product of the Greek words for "same" and "state" or "condition") and plasticity (or "change"), thus homeostatic plasticity means "staying the same through change."

\subsection{Rate-based Hebbian Learning}
\paragraph{Goal} is to find a mathematically formulated learning rule based on Hebb's postulate.\\
We begin by focusing on a single synapse with efficacy $w_{ij}$ transmitting signals from a pre-synaptic neuron j to a post-synaptic neuron i. The activity ((mean) firing rate - average number of spikes per unit time) of i is denoted by $v_i$ and that of j by $v_j$. Based on the (1) \texttt{locality} aspect of Hebbian Learning we can write a general description of the change of the synaptic efficacy. 
\[
\frac{d}{dt}w_{ij}=\mathbf{F}(w_{ij}; v_j^{pre}, v_i^{post})
\]
The membrane potential is further uniquely defined by the postsynaptic firing rate $v_i = g(u_i)$ with a monotone gain function g and therefore doesn't have to be included in F.\\
The (2) \texttt{cooperativity} aspect of Hebb's postulates implies that pre- and postsynaptic neuron have to be active simultaneously for a synaptic weight change to occur and this property can be used to learn something about the function F. Taylor series expansion around $v_i=v_j=0$ leads to:
\[
\frac{d}{dt}w_{ij}=c_0(w_{ij})+c_1^{post}(w_{ij})v_i+c_1^{pre}(w_{ij})v_j+c_2^{pre}(w_{ij})v_j^2+c_2^{post}(w_{ij})v_i^2+c_2^{corr}v_i v_j+\mathcal{O}(v^3)
\]
The term with $c_2^{corr}$ is bilinear in pre- and postsynaptic activity showing that the change of the synaptic efficacy $w_{ij}$ is indeed subject to a combination of changes in both activities.\\
Simplest choice of F is to fix $c_2^{corr}=c>0$ and set all other terms of the Taylor expansion to zero resulting in the prototype of hebbian learning.\\
\[
\frac{d}{dt}w_{ij}=c_2^{corr}v_i v_j
\]
This learning rule includes only first-order terms and therefore models non-Hebbian plasticity. More complicated learning rules include higher-order terms of the taylor expansion. If $c_2^{corr}=c<0$, the learning rule is anti-hebbian.\\
F is dependent on the efficacy $w_{ij}$, so that it will not, in reality, grow without limit. A saturation of synaptic weights can be achieved if parameter $c_2^{corr}$ goes to zero as $w_{ij}$ approaches its maximum value:
\[
	c_2^{corr}(w_{ij})=\gamma_2(1-w_{ij})
\]
Originally Hebb did not consider a rule for decreasing the synaptic weights. In order to have a feasible model however, the synaptic efficacy should decrease in the absence of stimulation.\\
\[
	c_0(w_{ij})=-\gamma_0 w_{ij}
\]
The combination results in the simplest feasible learning rule:
\[
\frac{d}{dt}w_{ij}=\gamma_2(1-w_{ij})v_i v_j-\gamma_0 w_{ij}
\]
\subsubsection{Gating}
Gating is the process of restricting the number of changes that occur. We discern between postsynaptic and presynaptic gating.
\paragraph{Postsynaptic gating} A weight change occurs only if the postsynaptic neuron is active, $v_i^{post}>0$. $\Rightarrow$ the weight changes are "gated" by the postsynaptic neuron. Only the efficacies of highly active presynaptic neurons $v_j^{pre}>v_{\phi}$, which is a function of $w_{ij}$, are strengthened.
\[
\frac{d}{dt}w_{ij}=\gamma v_i^{post}(v_j^{pre}-v_{\phi}(w_{ij})), \gamma>0
\]
\paragraph{Presynaptic gating} role of pre- and postsynaptic firing rate are exchanged. Now, a change in synaptic weights can only occur if the presynaptic neuron is active, $v_j^{pre}>0$, and the activity of the postsynaptic neuron determines the direction of the change.
\[
\frac{d}{dt}w_{ij}=\gamma v_j^{pre}(v_i^{post}-v_{\phi})
\]

\subsubsection{BCM rule}
The "Bienenstock-Cooper-Munroe" rule is a generalization of the presynaptic gating rule, where $\Phi$ is a non-linear function and the reference rate $v_\phi$ is the running average of the postsynaptic activity $v_i$. 
\[
\frac{d}{dt}w_{ij}=\eta \Phi(v_i^{post}-v_{\phi})v_j^{pre}
\]
Note that the dependency of $v_\phi$ on $w_{ij}$ (via $v_i^{post}$) implements a homeostatic plasticity.
%For low values of the postsynaptic activity (y<θM), ϕ is negative; for y>θM , ϕ is positive. The rule stabilizes by allowing the modification threshold, θM , to vary as a super-linear function of the previous activity of the cell. Unlike traditional methods of stabilizing Hebbian learning, this "sliding threshold" provides a mechanism for incoming patterns, as opposed to converging afferents, to compete.
%In order to avoid that the postsynaptic firing rate blows up or decays to zero, it is therefore necessary to turn  $ \nu_{\theta}^{}$ into an adaptive variable.
\subsection{Learning in Rate Models (functional consequences of hebbian learning)}
\paragraph{Goal:} understand how activity-dependent learning rules influence the formation of connections between neurons in the brain.\\
Plasticity is controlled by the statistical properties of the presynaptic input for the postsynaptic neuron.
\subsubsection{Evolution of synaptic weights} For the sake of simplicity we model the presynaptic input as a set of static patterns $\{x^\mu \in \mathcal{R}^N; 0\leq \mu \leq p\}$. At each time step one of the patterns is selected at random and the presynaptic rates are fixed to $v_i = x_i^\mu$. The synaptic weights are modified according to a Hebbian learning rule dependent on the correlation of pre- and postsynaptic activity: $\frac{d}{dt}w_{ij}=a_2^{corr}v_i^{post}v_j^{pre}$.\\
In a general rate model the firing rate $v_i^{post}$ is given by a nonlinear function of the total input: $v^{post}=g(\sum_i w_i v_i^{pre})$. For simplification reasons we focus on a linear rate model: 
\[
v_i^{post}=\sum_k w_{ik} v_k^{pre}
\]
We can now combine the learning rule and the linear rate model to form:
\[
\frac{d}{dt}w_{ij}=a_2^{corr} \sum_k w_{ik} v_k^{pre} v_j^{pre}
\]
We are interested in the long-term behaviour of the synaptic weights and therefore consider the expectation value of the weight vector, i.e., the weight vector averaged over the sequence $(x^{\mu_1}, x^{\mu_2}, ..., x^{\mu_n})$ that have so far been presented to the network.
\begin{align*}
&\left\langle \frac{d}{dt}w_{ij} \right\rangle = a_2^{corr}\sum_k \langle w_{ik} \rangle \langle \underbrace{x_k^\mu x_j^\mu}_{C_{kj}} \rangle
&C_{kj}=\langle x_kx_j \rangle = \frac{1}{p}\sum_{\mu=1}^p x_k^\mu x_j^\mu
\end{align*}
$C_{kj}$ are the correlation matrix components.

\subsubsection{Detection of correlation}
\label{sec:correlationDerivation}

We denote here the weight vector between the post- and presynaptic neuron as $\vec{w}$, its components are $w_i$ for $i$ from $1$ to $N$. (Note that we left out the second index because we are only considering a single postsynaptic neuron.)

From what we had before, we get:
%\paragraph{Weight update.} Considering a simple Hebbian learning rule, we have:
%\[\frac{dw_j}{dt} = a_2^{corr} v^{post} v_j^{pre}\]
%Now consider presenting pattern $\vec{x}^\mu$ during a time $\Delta t$. Substituting some, simplfying and ignoring $\Delta t$ just for the sake of succintness, we get:
%\[ \Delta w_j(\mu) = a_2 \sum_k w_k x_k^\mu x_j^\mu \]
%If we consider a batch update, we update the weight after all $p$ input patterns have been presented. We get:
%\begin{align*}
%\langle\Delta w_j \rangle &= \frac{1}{p}\sum_{\mu} \Delta w_j(\mu)\\
%&= a_2 \sum_k \underbrace{\left(\frac{1}{p} \sum_\mu x_k^\mu x_j^\mu\right)}_{C_{kj}} w_k
%\end{align*}
%In vector notation, with incorporation of $\Delta t$ and assuming that this is small, we get:
\[ \frac{d\vec{w}}{dt} = a_2 C \vec{w}\]
which is an ordinary matrix differential equation\footnote{Some information on the topic can be found on http://en.wikipedia.org/wiki/Matrix\_differential\_equation}.

\paragraph{Solving the differential equation}
We know that particular solutions to the equation have the form $\vec{w} = \alpha (t) \vec{e}_n$, with $\vec{e}_n$ the $n$-th eigenvector of $C$. The general solution is hence a sum of such terms, incorporating all eigenvectors.

Note also that $C$ is symmetric and as such has real eigenvalues. We can write $C\vec{e}_k = \lambda_k\vec{e}_k$, for $1\leq k \leq N$.
%Apparently the eigenvalues are also positive. This can only be the case when C is positive-definite. Why is this so?

With this knowledge we easily derive:
\begin{align*}
\frac{d\vec{w}}{dt} = \frac{d}{dt}\alpha(t)\vec{e}_n &= a_2 C \vec{e}_n \alpha(t) \\
&=a_2 \lambda_n \vec{e}_n \alpha(t)
\end{align*}
And:
\[\vec{w}(t) = \sum_n \exp(a_2 \lambda_n t) \vec{e}_n\]

\paragraph{Wrapping it up}
We know that the eigenvalues are both real and positive, so we can order them as follows:
\[ \lambda_1 \geq \lambda_2 \geq ... \geq \lambda_N > 0\]
Hence, we can write:
\[\vec{w}(t) = e^{\lambda_1 a_2 t} \sum_k \exp(a_2 (\lambda_k-\lambda_1) t) \vec{e}_k\]
It is immediately clear that since $\lambda_1$ is the largest eigenvalue, in the limit $\vec{w}$ will become parallel to $\vec{e}_1$, which is the principal component of the input set\footnote{In fact, this is only true when $\langle \vec{x}\rangle = 0$, in which case the covariance equals the correlation.}. 
Since the postsynaptic neuron firing rate is equal to the dot product of the input rates with the weight vector, the output is actually the projection of the input onto the principal component. It is the size of the input along the axis of maximal variance.
Hence, the neuron detects correlations in the input.


\subsubsection{PCA} Using PCA we can detect the direction of maximal variance in the set of patterns, hence the Hebbian learning rule basically implements PCA where only the first principal component is retained.

We consider $C$ to be the correlation matrix, as before. Now, also consider
\[C_{kj}^0 = \left\langle (x_k-\langle x_k \rangle)(x_j - \langle x_j \rangle)\right\rangle\]
i.e. $C^0$ is the covariance matrix of the input.

Now, let's denote the eigenvectors of $C^0$ as $\vec{e}_n$ and the corresponding eigenvalues as $\lambda_n$:
\[ C^0\vec{e}_n = \lambda_n \vec{e}_n \]
Let's assume that the eigenvectors are ordered and that the vector with largest associated eigenvalue is $\vec{e}_1$. This vector is called the \textbf{principal component}.

The PCA algorithm is summarized in Listing \ref{list:pca}.

\begin{lstlisting}[caption= PCA-algorithm, label=list:pca]
1. Substract mean
2. Calculate covariance matrix
3. Calculate eigenvectors
\end{lstlisting}

\paragraph{Dimensionality reduction} PCA can be used to perform a dimensionality reduction. In this case, the principal components of a data set are determined and only the first (i.e. most significant) components are retained.

\paragraph{Theorem: the first component has maximal variance.}
We can easily prove this as follows. Let's still consider $y$ to be the output of a neuron, as such being the dot product of the input and weight vectors. We now need to find the direction of $\vec{w}$ that will cause $y$ to have maximal variance.
The variance $V$ is defined as:
\begin{align*}
V = \langle y^2 \rangle &= \langle (\vec{w}\cdot\vec{x})^2\rangle \\
&=\vec{w}^T \langle\vec{x}\vec{x}^T\rangle\vec{w} \\
&= \vec{w}^T C^0 \vec{w}
\end{align*} 
Now assume $\vec{w}$ is normalized and $\vec{e}_k$ are the normalized eigenvectors of $C^0$. Since $C^0$ is symmetric (see above), its eigenvectors form a orthogonal basis. Hence, we can always write:
\[\vec{w} = \sum \alpha_k \vec{e}_k \]
And since $\vec{w}$ is normalized, we have
\[\sum \alpha_k^2 = 1\]
Hence:
\[ V = \sum \alpha_k^2\vec{e}_k^T C^0 \vec{e}_k = \sum \alpha_k^2\lambda_k\]
We can easily see that since $\lambda_1$ is the largest eigenvalue, the largest variance will be achieved when $\vec{w} = \vec{e}_1$ (because of the normalization). This proves the theorem.

We could also try to solve the problem of finding the first principal component by using Lagrange multipliers. The method of Lagrange multipliers provides a strategy for finding the local maxima and minima of a function subject to equality constraints. Consider the optimization problem maximize $f(\omega)$, subject to $g(\omega) = c$. Both $f,g$ need to have continuous first partial derivatives. A new variable, $\lambda$ called Lagrange multiplier is introduced and the Lagrange function is defined by
\[
	L(\lambda, \omega) = f(\omega) - \lambda(g(\omega) - c)
\]
If $\omega_0$ is a maximizer of $f(\omega)$, for the original constraint problem, then there exists $\lambda_0$ such that $(\omega_0, \lambda_0$ is a stationary point for the Lagrange function. In the case of PCA, the function we'd like to maximize is the variance. If the data has zero-mean, we can write
\[
	f(\omega) = < y^2 > = < (\omega^T * x)^2 > = \omega^T <x x^T> \omega = \omega^T C \omega
\]
And we can observe that the optimal $\omega$ is an eigenvector of the covariance matrix C. Among all the eigenvectors, $\omega^*$ is the one corresponding to the largest eigenvalue, as we try to maximize $\omega^T C \omega$ which is equal to $\lambda$.

\subsection{Oja's rule}
The Oja learning rule is a mathematical formalization of the Hebbian learning rule, such that over time the neuron actually learns to compute a principal component of its input stream. The forgetting term added to balance the growth of the weights this time is not only proportional to the value of the weight, but also to the square of the output of the neuron $v_i$.
\[
\frac{d}{dt}w_{ij}=a_2^{corr}v_j^{pre}v_i^{post}-\underbrace{a_2^{post}}_{a_2^{corr}w_{ij}}(v_i^{post})^2=a_2^{corr}(v_j^{pre}v_i^{post}-w_{ij}(v_i^{post})^2)
\]
Now, the forgetting term balances the growth of the weight. The squared output $(v_i^{post})^2$ guarantees that the larger the output of the neuron becomes, the stronger is this balancing effect.

This has the result that the norm of the weight vector tends to one. Compare this to the result obtained in Section \ref{sec:correlationDerivation}, where we ended up with weights that grew exponentially without bound.
\subsubsection{Oja's rule and PCA}
\[
v_i^{post}=\mathbf{w}^T\mathbf{x}^\mu
\]
\[
\frac{d}{dt}\mathbf{w}=a_2^{corr}(\mathbf{x}^\mu \mathbf{x}^{\mu T}\mathbf{w}-\mathbf{w}^T\mathbf{x}^\mu \mathbf{x}^{\mu T}\mathbf{w}\mathbf{w}), \{x^\mu, 0 \leq \mu \leq p\}
\]
This is the incremental change for just one input vector $x^\mu$. When the algorithm is run for a long time, changing the input vector at every step, one can look at the average behaviour. An especially interesting question is what the value of the weights is when the average change in weights is zero. This is the point of convergence of the algorithm. Averaging the right hand side over $x^\mu$ while $w$ stays constant yields the following equation.
\[
\frac{d}{dt}\mathbf{w}=a_2^{corr}(\underbrace{\langle \mathbf{x}\mathbf{x^T} \rangle}_{C} \mathbf{w}-\underbrace{(\mathbf{w}^T\underbrace{\langle \mathbf{x}\mathbf{x^T} \rangle}_{C} \mathbf{w}}_{\lambda})\mathbf{w})=0, \overline{x}=0
\]
Considering that the quadratic form $w^TCw$ is a scalar, this equation clearly is the eigenvalue-eigenvector equation for the covariance matrix C. This analysis shows that if the weights converge in the Oja learning rule $C\mathbf{w}=\lambda\mathbf{w}$, then the weight vector becomes one of the eigenvectors of the input covariance matrix $\mathbf{w}=\mathbf{e_1}$ (\textsc{only this state is stable}), and the output of the neuron becomes the corresponding principal component. 

Principal components are defined as the inner products between the eigenvectors and the input vectors. For this reason, the simple neuron learning by the Oja rule becomes a principal component analyzer (PCA).
Although not shown here, it has been proven that the neuron will find the the \texttt{first principal component} (as shown in the second exercise session) and the norm of the weight vector tends to one (can be found on Wikipedia\footnote{http://en.wikipedia.org/wiki/Oja's\_rule under 'Derivation'}).
\textsc{PCA is used for Hebbian learning in \textbf{linear} neurons}

\subsection{Independent Component Analysis (ICA)}

\paragraph{Goal:} Compute a transformation $\mathbf{y}=A\mathbf{x}$ such that the entries $y_i$ of the transformed vector (components) are statistically independent. Statistical independence is a stronger constraint than uncorrelatedness required by the PCA and actually implies it. (The two conditions are equivalent \textit{only} if the sources $x_i$ are generated from Gaussian random variables - one random vector). 

\paragraph{Assumptions:} A random input vector $\mathbf{x}$ is generated by a linear combination of statistically independent $p(\mathbf{x})=p(x_1,\dots,x_n)=p_1(x_1)\dots p_n(x_n)$ and stationary components (sources), $\mathbf{x}=A\mathbf{y}$. The task is now is to find a matrix W so as to recover the original signal sources, the components of $\mathbf{y}=\begin{bmatrix}s_1 & s_2 & \dots & s_n\end{bmatrix}^T$ by exploiting the information hidden in the samples of the mixed signal patterns $\mathbf{x}$ (blind source separation). $\mathcal{A}$ is known as the mixing and $W$ as the demixing matrix, respectively.\\
In contrast to PCA which can always be performed, ICA is meaningful only if the involved random variables are non-Gaussian. Each of the resulting independent components is \texttt{uniquely} estimated up to a multiplicative constant. For this reason the components are considered to be of unit variance. (Multidimensional Gaussians are isometric/rotationally symmetric and therefore the orientation/principal axis of the data can't be recovered). % source Stanford ML lecture 15: http://www.youtube.com/watch?v=QGd06MTRMHs, from 1h
Searching for independent rather than uncorrelated features gives us the means of exploiting a lot more information, hidden in the higher order statistics of the data.
The central limit theorem plays a crucial role in the case of a mixture of independent random variables. The distribution of a sum of independent random variables tends towards a gaussian distribution. Hence, we try to maximize the non-gaussianity of the independent variables.
Independence is another crucial fact: Source signals are independent, but the signal mixtures are not independent anymore, as they are built from the same source signals.

\paragraph{Ambiguities}
One application of ICA is to the cocktail party syndrome: We hear a mixture of many different sources. With ICA, we can try to find the sources.
permutation of speakers (ICA can't tell which is which)
sign of the speakers (ICA can't guarantee outputting the right sign of the signal)
for many applications (audio) one doesn't care about these facts
% Standford ML course

\paragraph{Preprocessing (whitening)} \begin{itemize}
\item Zero mean data
\item Whitening (unit variance). Perform PCA on mixed signals and divide each component by the square-root of its eigenvalue $x_k^n=\frac{1}{\sqrt{\lambda_k}}x_k \Rightarrow C^n=\begin{pmatrix}1 & 0 \\ 0 & 1\end{pmatrix}$. After PCA entries of projections $\mathbf{y}$ of random vectors are uncorrelated and independence can be rewritten to that of projections $p(y_1,\dots y_n)=p_1(y_1)\dots p_n(y_n)$
\end{itemize}
How to find the natural axis? We need a measure for non-gaussianty.
If we look at two-dimensional data, an arbitrary projection will be close to a Gaussian distribution which doesn't tell us anything about our data (neuronal inputs). $\Rightarrow$ search for direction which is maximally non-Gaussian.\\
\subsubsection{Gaussianity vs. non-gaussianity}
This approach in performing ICA is a direct generalization of the PCA technique. The Karhunen-Lo\`{e}ve transform focuses on the second-order statistics and demands the cross-correlations $\langle y_iy_j \rangle$ to be zero. In ICA demanding that the components of $\mathbf{y}$ be statistically independent is equivalent to demanding all the higher order cross-cumulants to be zero (going up to the fourth-order cumulants is sufficient for many applications).%A Gaussian pdf is the least interesting pdf. The Gaussian is the most "random" of all pdfs
% source: Pattern Recognition, Chapter 6, Theodoridis, Koutroumbas
For Gaussians, the PCA is equal to ICA, as the normal distribution is only characterized by its zero, first and second moment. Gaussians are therefore statistically independent if the covariance is zero. For other distributions, this is not the case and variables are only decorrelated but not statistically independent if the covariance is equal to zero. Hence, in order to separate the variables, we need to apply ICA.
careful: Cross-cumulant vs auto-cumulant
cumulants vs. moments? These are actually moments?
\begin{align*}
&\mathcal{K}_1(y(i))=\E[y(i)]=0,\quad \text{\textbf{mean}}\\
&\mathcal{K}_2(y(i)y(j))=\E[y(i)y(j)],\quad \text{\textbf{(Co)variance}}\\
&\mathcal{K}_3(y(i)y(j)y(k))=\E[y(i)y(j)y(k)],\quad \text{\textbf{skewness}}
\end{align*}
\begin{align*}
\mathcal{K}_4(y(i)y(j)y(k)y(r))&=\E[y(i)y(j)y(k)y(r)]-\E[y(i)y(j)]\E[y(k)y(r)]\\
&-\E[y(i)y(k)]\E[y(j)y(r)]-\E[y(i)y(r)]\E[y(j)y(k)]\\
&=\E[y^4]-3\E[y^2],\quad \text{\textbf{kurtosis}}
\end{align*}
We need a measure for non-gaussianity, which we find in the higher order cumulants, because for Gaussian distributions there is no difference in ICA \& PCA. The kurtosis (4-th order cumulant) is such a measure of non-gaussianity. The smaller it is the more it describes a leptokurtic curve with large values around the mean descending fast towards the first standard deviation. The problem can now be reduced to finding a matrix, W, for that the second-order (covariances) and fourth-order cross-cumulants, kurtosis, of the transformed patterns are zero.\\
The kurtosis is zero for gaussians. For super-gaussian variables, the kurtosis is $ > 0$, whereas a sub-gaussian variable has its kurtosis $< 0$. This has implications in how to recover the source. In the case of super-gaussian variables, we need to maximize the kurtosis, in the case of sub-gaussian variables, we need to minimize!
\textsc{Does anyone understand the meaning of the following equation?}
\[
J(w)=[\E(F(y))-\E(F(v))]^2
\]
\textsc{TODO: maximize non-gaussianity}

\subsubsection{Blind Source separation}
Separation of a set of source signals from a set of mixed signals, without the aid of information about the source signals or the mixing process. This is one application field of ICA.

\subsubsection{ICA by neuronal rule}
batch
\[
\Delta \mathbf{w}=\eta\E\left[\mathbf{x}g(\mathbf{w}^T\mathbf{x})\right]
\]
online
\[
\Delta \mathbf{w}=\eta \mathbf{x} g(\mathbf{w}^T\mathbf{x})
\]
\textsc{TODO}

\subsubsection{FastICA}
\begin{enumerate}
\item Initialize w
\item Newton step $\mathbf{w}^{new}=\E \left[\mathbf{x}g(\mathbf{w}^T\mathbf{x})-\mathbf{w}g'(\mathbf{w}^T\mathbf{x})\right]$
\item Renormalize $\mathbf{w}=\frac{\mathbf{w}^{new}}{\vectornorm{\mathbf{w}^{new}}}$
\item if algorithm has not converged, go to 2
\end{enumerate}
\subsubsection{Temporal ICA}
\paragraph{Goal:} Find unmixing matrix W such that outputs of time-dependent signals are independent. 
\paragraph{Assumptions:} Signals are time-dependendent and the different components may be recorded with delays $\tau_i$
\[
	\langle y_i(t)y_k(t-\tau_k) \rangle = \delta_{ik}\lambda(\tau)
\]
\section{Competitive Learning}

\subsection{Clustering}
\subsubsection{Nearest Neighbour}
Initialize prototypes $p_k$ to represent groups of data points. Assign datapoints to nearest neighbouring prototype $\abs{\mathbf{p}_k-\mathbf{x}}\leq\abs{\mathbf{p}_i-\mathbf{x}}$ resulting in Voronoi tesselation of input space. For data compression the input $\mathbf{x}$ is classified and the index k transmitted. This gives rise to the construction error
\[
\E(\mathbf{w}_1, \dots, \mathbf{w}_n) = \sum_k \sum_{\mu \in C_k} (\mathbf{w}_k-\mathbf{x}^\mu)^2
\]
But how to choose initialization of prototypes? To minimize the reconstruction error one has discretize the input space adaptively.
\subsubsection{K-Means}
Standard K-means is based on a random initialization of prototypes. 
\begin{enumerate}
\item For every data pattern $x^\mu$ the winning prototype $\mathbf{p}_k$is determined with the nearest neighbour rule.
\item The winning prototype is moved closer to the classified $\mathbf{x}_k^\mu$ by updating it according to $\Delta \mathbf{p}_k = \eta (\mathbf{p}_k-\mathbf{x}^\mu$).
\end{enumerate}
\paragraph{Showing that the error decreases}
Gradient descent on the error surface in the following manner 
\[
\Delta \mathbf{p}_j = -\eta \frac{\partial\E}{\partial\mathbf{w}_j} = -\eta \frac{\partial}{\partial\mathbf{w}_j} \sum_k \sum_{\mu \in C_k} (\mathbf{w}_k-\mathbf{x}^\mu)^2
\]
\begin{align*}
&\text{\textbf{batch update}}: \quad\Delta \mathbf{p}_j
= -2\eta \sum_{\mu \in C_j}(\mathbf{x^\mu}-\mathbf{w}_j)\\
&\text{\textbf{online update}}: \quad\Delta \mathbf{p}_j
= 2\eta (\mathbf{x^\mu}-\mathbf{w}_j)
\end{align*}
In the batch update rule distances from the data points in the prototype's class are summed up and the prototype is moved towards the mean with step size $\eta$.\\
After convergence: $\Delta w_k=0$
\begin{align*}
&\sum_{\mu \in C_k}\mathbf{w}_k-\sum_{\mu \in C_k}\mathbf{x}^\mu=N_k\mathbf{w}_k-\sum_{\mu \in C_k}\mathbf{x}^\mu\\
&\Rightarrow \mathbf{w}_k=\frac{1}{N_k}\sum_{\mu \in C_k}\mathbf{x}^\mu
\end{align*}
So with the batch update rule each prototype is at the center of its data cloud after convergence.\\
The online update has a stochastic component, because one has to choose a data point at random and move into its direction by the step size $\eta$. This causes the algorithm to jitter around the mean. To alleviate this problem the trick is to reduce $\eta$ so that the algorithm eventually freezes.
\paragraph{Problem of dead units}
Dead units are prototypes without data clouds. The error surface allows the algorithm to converge into a state permitting dead units through several local minima. To overcome this problem it is useful to initialize the prototypes to data points in k-means.

\subsection{Neuronal Implementation of Clustering}
(now multiple postsynaptic neurons for the same input?)
In the neuronal implementation the data patterns take the form of sensory inputs to neurons in the brain, one postsynaptic neuron for every prototype $\mathbf{p}_j$. The activity of these neurons is defined by a non-linear neuron-model $v_1^{post}=y_1=g(\sum_k \mathbf{w}_{ik}\mathbf{x}_k^\mu)=g(\mathbf{w}^T\mathbf{x}^\mu)$ which performs a non-linear transform of the scalar product of the prototypes used as weights $\mathbf{w}_j=\mathbf{w}_k$ which are now synaptic weights and the sensory inputs.\\
For \textsc{normalized weight vectors} the clustering expression $\abs{\mathbf{p}_k-\mathbf{x}}\leq\abs{\mathbf{p}_i-\mathbf{x}}$ can be rewritten to $\mathbf{w}_k^T\mathbf{x}\leq\mathbf{w}_k\mathbf{x}$. This means that the smallest distance to the prototype $\mathbf{w}_k$ results in the largest scalar product. We reformulate our clustering aim to finding the postsynaptic neuron with the maximal scalar product ("biggest total stimulation").

\paragraph{Winner take all circuit}
How can it be assured that only one neuron wins, that is to say that the respective input pattern is classified to a single prototype? Simply by introducing a \texttt{lateral term} incorporating activities of the other postsynaptic neurons for the other prototypes. The winner is the neuron that receives the most stimulation from outside (strongest outside drive) inhibiting the others which is equivalent to saying input pattern is classified to the prototype with which it forms the largest scalar product $\mathbf{w}_k\leq\mathbf{x}^\mu$.
\begin{align*}
&y_i(t+1)=g(\text{total drive})=g(\sum_k w(t+1)_{ik}\mathbf{x}_k^\mu+\sum_j B_{ij}y(t)_j),\quad B_{ij}<0\\
&\Rightarrow y_i(t^{final})=\left( \begin{array}{cc} \text{1 for winner}\\\text{0 for others}\end{array} \right)
\end{align*}
How to update weights?

\paragraph{Hebbian Learning Rule (no inhibition)}
For every pattern $\mathbf{x}^\mu$ find winning prototype $\mathbf{w}_k=\mathbf{p}_k$ and update it according to the following learning rule:
\begin{align*}
\frac{d}{dt}w_{ki}&=\eta(y_k x_i^\mu-\gamma w_{ki}y_k)\\
&=\eta(\delta_{ki} x_i-w_{ki}\delta_{ki})\\
&=\eta(x_i-w_{ki})%\\\text{Why can y_k be regarded constant?}
\end{align*}
update for winner k (only neuron active) is same eq. as before (k-means), now derived as a hebbian learning rule:
\[
\frac{d}{dt} \mathbf{w}_{k}=\eta(\mathbf{x}^\mu-\mathbf{w}_{k})
\]

\paragraph{Kohonen Learning Rule}
\begin{enumerate}
\item choose input $\mathbf{x}^\mu$
\item determine winner k according to current weights for input (scalar product)
\item update weights of winner k \[
\frac{d}{dt} w_{ki}=\eta(y_kx_i^\mu-\gamma w_{ki}y_k)
\]
and neighbours j
\[
\frac{d}{dt} = \eta \Lambda(j,k)(\mathbf{x}^\mu-\mathbf{w}_j),\quad \text{on-line}
\] Every neighbour j is inhibited according to inhibition coefficients of k for j.
\item decrease size of neighbourhood function $\Lambda(j,k)$ to ensure convergence?
\item reiterate from 1) until stabilization
\end{enumerate}
Thus, the neuron whose weight vector was closest to the input vector is updated to be even closer. The result is that the winning neuron is more likely to win the competition the next time a similar vector is presented, and less likely to win when a very different input vector is presented. As more and more inputs are presented, each neuron in the layer closest to a group of input vectors soon adjusts its weight vector toward those input vectors. Eventually, if there are enough neurons, every cluster of similar input vectors will have a neuron that outputs 1 when a vector in the cluster is presented, while outputting a 0 at all other times. Thus, the competitive network learns to categorize the input vectors it sees.

\section{Reinforcement learning}

\subsection{Introduction}
\paragraph{RL vs. supervised learning}
Supervised learning, used in statistical pattern recognition and artifical neural networks is learning from examples provided by a knowledgable external supervisor whereas in RL an agent is instructed to learn from its own experience. It is a ''behavioral'' learning problem: Through interaction between the learning system and its environment, whereas the system seeks to achieve a specific goal.

\paragraph{trade-off exploration vs. exploitation}
An agent has to exploit what it \textsc{already knows} in order to obtain a reward, but it also has to \textsc{explore} in order to make \textsc{better action selections} in the future. The agent must try a variety of actions and progressively favour those that appear to be best, because neither schemes can be pursued exclusively without failing a the task. Estimate reward probabilities vs. take action to maximize reward.

\paragraph{RLs main characteristic}
\textsc{Not one-shot decision making problem}, harder than supervised learning
RL explicitly considers the \textsc{whole} problem of a goal-directed agent \textsc{interacting} with an uncertain environment. All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments. Moreover, it is usually assumed from the beginning that the agent has to operate despite significant \textsc{uncertainty about the environment} it faces. When reinforcement learning involves \textsc{planning}, it has to address the \textit{interplay between planning and real-time action selection}, as well as the question of how environmental models are acquired and improved.\\
An agent's actions are permitted to affect the future state of the environment (e.g., the next chess position, the level of reservoirs of the refinery, the next location of the robot), thereby affecting the options and opportunities available to the agent at later times. Correct choice requires taking into account indirect, delayed consequences of actions, and thus may require foresight or planning. The knowledge the agent brings to the task at the start--either from previous experience with related tasks or built into it by design or evolution--influences what is useful or easy to learn, but interaction with the environment is essential for adjusting behavior to exploit specific features of the task.

\subsubsection{Elements of reinforcement learning}
there are four main subelements of a reinforcement learning system.

\paragraph{Policy}
commonly denoted by $\pi$, it defines the learning agents way of behaving at a given time. Mapping from the perceived states of the environment to the actions that present themselves to the agent in these states. The policy alone is sufficient to determine the behaviour of an agent. In oder words, a policy is a rule used by the learning system to decide what to do, given the current state of the environment.

\paragraph{Reward function}
defines the goal in a reinforcement learning problem. Maps each perceived state of the environment to a single number, a reward, which describes the intrinsic desirability of the state. The agent's sole objective is to maximize its total gain from the rewards it receives.\\
The reward function may serve as a basis for altering the agent's policy.

\paragraph{Value function}
While the reward function indicates which actions are immediately beneficial to the agent, a \textbf{value function} specifies what is good for the agent in the long run. The value of a state is the \textsc{expected} total amount of rewards the agent will accumulate from this state onwards.
For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.

\paragraph{Model of the environment}
A model mimics the behaviour of the environment. \textit{For example, given a state and action, the model might predict the resultant next state and next reward}. Models are used for \textsc{planning}, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. The incorporation of models and planning into reinforcement learning systems is a relatively \textsc{new development}. Early reinforcement learning systems were explicitly trial-and-error learners; what they did was viewed as almost the opposite of planning. Nevertheless, it gradually became clear that reinforcement learning methods are closely related to dynamic programming methods, which do use models, and that they in turn are closely related to state-space planning methods. 

%\subsection{Evolutionary methods}
%Genetic algorithms, genetic programming, simulated annealing, and other function optimization methods have also been used to solve reinforcement learning problems. These methods search directly in the space of policies without ever appealing to value functions. We call these \textbf{evolutionary methods} because their operation is analogous to the way biological evolution produces organisms with skilled behaviour even when they do not learn during their individual lifetimes (organisms that have chosen adaptive policies survive). Effective when space of policies is sufficiently small or accurately sensing the state of the environment is impossible.

\subsection{Markov Decision Problems (MDP)}
Reinforcement learning algorithms model the world as a Markov Decision Problem (MDP) 
\begin{align*}
\mathbf{MDP}&\left(S, A, P_{s\rightarrow S'}^a, \gamma, \mathcal{R}\right)\\
&S,\quad \text{state space}\\
&A,\quad \text{contains possible actions associated with states}\\
&\sum_{s'}P_{s\rightarrow s'}^2=1,\quad\text{outgoing action prob. sum to 1}\\
&\gamma, \quad\text{discount factor for future rewards}\\
&\mathcal{R}:S\rightarrow\mathbb{R}, \quad \text{reward function maps states to reward values}\\
&\text{policy/strategy profile}\quad \pi(s,a), \quad \text{describes which action a to take in state s}
\end{align*}

\subsection{Reward-based action learning: Q-values}
In the case of a \texttt{1-step horizon} the agent starts of in an initial state $s$ from where he can get to several states $s_i'$ by choosing an associated action $a_i$. A reward $P_{s \rightarrow s'}^a$ is associated with every action.
\paragraph{Goal:} The agent should find the optimal strategy $a^*$ resulting in the maximization $Q(s,a^*) > Q(s, a_j)$ of the expected reward $Q(S,a)=\sum_{s'} P_{s \rightarrow s'}^a R_{s \rightarrow s'}^a$.

\paragraph{Update rule:}
The Q values are however not known at the beginning, because the reward probabilities $P_{s \rightarrow s'}^a$ are not known. Therefore, the agent needs to learn them via iterative updates: 
\[
\Delta Q(s,a) = \eta\left[R_{s \rightarrow s'}^a-Q(s,a)\right]=\eta\left[r-Q(s,a)\right]
\]
These consist of the difference between received reward and expected reward multiplied by a learning rate $\eta$. Over multiple episodes the Q-values converge to the true value of the reward $r$, expressed by convergence of the mean update $<\Delta Q(s,a)> = 0$.

\subsection{SARSA and Bellman equation}
For \texttt{Multi-step horizon} reward-based action learning we reformulate the update rule to take into account the Q-values from the next step multiplied by a discount factor $\gamma$:
\begin{align*}
	\Delta Q(s,a) &= \eta \left[r-(Q(s,a) - \gamma Q(s', a'))\right]\\
&=\eta \underbrace{\left[r+\gamma Q(s', a')-Q(s,a)\right]}_{\delta_t}= \eta \delta_t
\end{align*}
This update rule is called \textbf{SARSA} for \texttt{State Action Reward State Action} because it uses the state and action of time t along with the reward, state, and action of time t + 1 to form the so-called \textbf{temporal difference (TD) error} $\delta_t$.
This update rule is called \texttt{associative} because the values that it learns are associated with particular states in the environment $s \in S$. We can write the same update rule without any s terms. This update rule would learn the value of actions without associating them with any environment state, thus it is \texttt{nonassociative}. \textit{For instance, if a learning agent were playing a slot machine with two handles, it would be learning only action values, i.e. the value of pulling lever one versus pulling lever two.} The associative case of learning state-action values is commonly thought of as the full reinforcement learning problem, although the related state value and action value cases can be learned by the same TD method.

\paragraph{Bellman equation}
The Bellman equation expresses the expected reward $Q(s,a)$ recursively over an infinite/multi-step horizon: 
\[
	Q(s,a) = \sum_{s'}P_{s \rightarrow s'}^a [R^a_{s \rightarrow s'} + \gamma \sum_{a'} \pi(s',a')Q(s',a')]
\]
The $Q(s',a')$ are weighted differently according to the chosen policy $\pi(s,a)$ derived from $Q(s,a)$.

\paragraph{SARSA algorithm}
The Bellman equation can be used for the SARSA algorithm implementing the SARSA update rule:
\begin{enumerate}
	\item Being in state s, chose action a according to \textbf{policy}
	\item Observe reward r, next state s'
	\item Choose action a' in state s' according to \textbf{policy}
	\item Update $\Delta Q(s,a)$
	\item $s' \rightarrow s$, $a' \rightarrow a$
	\item Goto 1)
\end{enumerate}

\subsection{Strategies for exploration vs. exploitation}
There are different strategies for exploitation/\texttt{ways to find optimal policy} while correct Q-values are not known:
\subsubsection{Choosing a policy}
The action choices for all states $s \in S$ are specified by a policy $\pi$. A policy consists of a rule for choosing the next state based on the value or predictions for possible next states (and in our case actions). More specifically, a policy maps state values to actions. Policy choice is very important due to the need to balance exploration and exploitation. 
%The RL agent is trying to accomplish two related goals at the same time: it is trying to learn the values of states or actions and it is trying to control the environment. Thus, initially the agent must explore the state space to learn approximate value predictions. 
This often means taking suboptimal actions in order to learn more about the value of an action or state. 
% Furthermore, constant exploratory action is necessary in nonstationary tasks in which the reward values actually change over time.
\begin{itemize}
	\item \textbf{Greedy policy}: Consider an agent who takes the highest-valued action $a^*,\quad Q(s,a^*)>Q(s,a)$ at every step. This policy is called a greedy policy, because the agent is only exploiting its current knowledge of state or action values to maximize the expected reward and is not exploring to improve its estimates of other states. \textit{A greedy policy is likely undesirable for learning because there may be a state which is actually good but whose value cannot be discovered because the agent currently has a low value estimate for it, precluding/preventing its selection by a greedy policy.}
	\item \textbf{$\varepsilon$-greedy policy}: take action which looks best with . A common way of balancing the need to explore states with the need to exploit current knowledge in maximizing rewards is to follow an $\varepsilon$-greedy policy which simply follows a greedy policy with probability $P = 1-\varepsilon$ and takes a random action with probability $\varepsilon$. \textit{This method is quite effective for a large variety of tasks.} 
	\item \textbf{Softmax strategy}: take action $a'$ which looks best with probability 
	Another common policy is softmax. You may recognize softmax as the stochastic activation function for Boltzmann machines, often called the Gibbs or Boltzmann distribution. With softmax, the probability of choosing action $a$ at time t is 
	\[
	P(a') = \frac{\exp{(\beta Q(a'))}}{\sum_a \exp{(\beta Q(a))}}
	\]
	where the denominator sums over all the exponentials of all possible action values and $\tau=\frac{1}{\beta}$ is the temperature coefficient. A high temperature causes all actions to be equiprobable, while a low temperature skews the probability toward a greedy policy. \textit{The temperature coefficient can also be annealed over the course of training, resulting in greater exploration at the start of training and greater exploitation near the end of training.} 
	\item \textbf{Optimistic greedy}: start off with Q values that are too big
\end{itemize}

\paragraph{Action Values Q versus State Values V}
\textsc{expected payoffs}
Almost all reinforcement learning algorithms are based on estimating value functions. These are functions of states V (or of state-action pairs Q) that estimate how good it is for the agent to be in a given state $V_s$(or
how good it is to perform a given action in a given state $Q_a$). The notion of ``how good" here is defined in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are defined with respect to particular policies.\\

Recall that a policy $\pi$ is a mapping from states $s \in S$ and actions $a \in A$ to the probability $\pi(s,a)$ of taking action $a$ when in state s. Informally, the value of a state s under a
policy $\pi$, denoted by $V^\pi(s)$ is the expected return when starting in s and \textsc{following $\pi$ thereafter}. % source Sutton and Barto: 3.7 Value Functions
The Bellman equation for state values V becomes:
\begin{align*}
	V^{\pi}(s)&=\sum_a \pi(s,a) \overbrace{\sum_{s'} P^a_{s \rightarrow s'} [R^a_{s \rightarrow s'} + \gamma \underbrace{\sum_{a'}\pi(s',a')Q(s',a')}_{V^\pi(s')}]}^{Q(s,a)}\\
	&=\sum_a \pi(s,a) \sum_{s'} P_{s \rightarrow s'}^a[R_{s \rightarrow s'}^a + \gamma V^{\pi}(s')],\quad\text{in form of itself}
\end{align*}
The difference between $Q$ and $V$ is the following: The value function is the expected return starting from state $s_t$ and following the policy $\pi$. The state-action value function $Q(s,a)$ is the expected return starting from state $s_t$, taking action $a$ and \texttt{only thereafter} following policy $\pi$.
\textsc{We however use the state-action values $Q$ instead of the mere state values $V$}

\subsection{On-policy vs off-policy learning}
\textsc{TODO: part doesn't fully match slides?}\\
On-policy means SARSA; off-policy means Q-learning. The difference lies in the method used to compute the update for $Q(s,a)$. While for SARSA, we use the policy $\pi$ to select the action $a'$ for the discounted term ($\gamma \sum_{a'} \pi(s',a')Q(s',a')$), the Q-learning algorithm simply uses the greedy approach to select $a'$. Off-policy Q-learning by default uses a greedy update rule and the policy derived from $Q$ is only employed for selection of the first action a from the current state:
\[
\Delta Q(s,a)=\alpha\left[r(a)+\gamma\underset{a'}{\text{max}} Q(s',a')-Q(s,a)\right]
\]
The \texttt{Problem} with these algorithms is that learning is slow, that is to say the propagation of Q-values is slow.

\subsection{Eligibility traces}
An eligibility trace is a temporary record of the \texttt{occurrence of an event}, such as the visiting of a state or the \texttt{taking of an action} (relevant in our case). The trace marks the memory parameters associated with the event as eligible for undergoing learning changes by weighting the learning rule accordingly.\\ 
%The more theoretical view is that they are a bridge from Temporal differences (TD) to Monte Carlo methods.
At the moment of the reward update, we want to also update previous action values along trajectories, because otherwise the information diffusion across several states takes too much time.
Every time an action $a$ is chosen according to a policy, the eligibility traces are evaluated thus: 
\begin{itemize}
	\item If a = action taken in state j: $e(j,a)(t) = \gamma\lambda e(j,a)(t-\Delta t) + 1$
	\item else: $e(s,a)(t) = \gamma \lambda e_(s,a)(t-\Delta t)$ 
\end{itemize}
The $\gamma$ here is the memory reduction parameter.\\
Then, for all states s and actions a use the respective eligibility trace as an additional coefficient for the SARSA update rule with TD error $\delta_t=r_{t}+\gamma Q(s',a')-Q(s,a)$.
\[
	\Delta Q(s,a) = \eta \delta_t e(j,a)
\]

\subsection{Continuous states}
Q-values for continuous states approximation by a weighted sum of basis functions
% needs some more explanation, I didn't fully get it.
\[
	Q(s,a) = \sum_j w_{aj}*r_j(s) = \sum_j w_{aj}*\phi(s-s_j)
\]
$s_j$ center of basis function, $\Phi$ shape of basis functions - determines how much of the weights are used.\\
If the time interval between subsequent weight updates approaches zero, the difference between two consecutive states $s' - s = v(a) * \Delta t$, where $v(a)$ is the velocity resulting from the choice of the action a. $s'-s$ approximates zero, therefore $s' \approx s$. Similarly, the finer the time resolution becomes, the likelier it is that $a \approx a'$. Thus, for small time steps, an online gradient descent on the error term $E_t$ becomes closer to a $\delta_t$ modulated Hebbian rule. 
\textsc{TODO: Exercise} $\frac{dQ(s',a')}{dw_{aj}}$

\subsection{Temporal Difference TD($\lambda$) algorithms}
% sources: http://www.scholarpedia.org/article/Temporal_difference_learning and http://www.stanford.edu/group/pdplab/pdphandbook/handbookch10.html#x26-1320009.2
TD algorithms are often used in reinforcement learning to predict a measure of the total amount $r_{tot.}$ of reward expected over the future, but they can be used to predict other quantities such as the expected reward $Q$ as well. Eligibility traces are usually implemented in this context by an exponentially-decaying memory trace, with decay parameter $\lambda$. This generates a family of TD algorithms $\mathbf{TD}(\lambda)$, $0\leq\lambda\leq 1$  with $\mathbf{TD}(0)$ corresponding to updating only the immediately preceding prediction (as described above - scholarpedia), and $\mathbf{TD}(1)$ corresponding to equally updating all the preceding predictions (\texttt{which is actually Q-learning}). %This also applies to non lookup-table versions of TD learning, where traces of the components of the input vectors are maintained. 
\begin{align*}
e(j,a)(t)&=\gamma\lambda e(j,a)(t-\Delta t)+\left\{\begin{array}{ll} 1*r_j & \text{if}\quad a=a'\\ 0 & \text{else}\end{array}\right.\\
&=\gamma\lambda e(j,a)(t-\Delta t)+r_j\delta_{a,a'}
\end{align*}
\textit{Eligibility traces do not have to be exponentially-decaying traces, but these are usually used since they are relatively easy to implement and to understand theoretically.}

%While there are a variety of techniques for unsupervised learning in prediction problems, we will focus specifically on the method of Temporal-Difference (TD) learning (Sutton, 1988). In supervised learning generally, learning occurs by minimizing an error measure with respect to some set of values that parameterize the function making the prediction. In the connectionist applications that we are interested in here, the predicting function is realized in a neural network, the error measure is most often the difference between the output of the predicting function and some prespecificed target value, and the values that parameterize the function are connection weights between units in the network. For now, however, we will dissociate our discussion of the prediction function from the details of its connectionist implementation for the sake of simplicity and because the principles which we will explore can be implemented by methods other than neural networks. In the next section, we will turn our attention to understanding how neural networks can implement these prediction techniques and how the marriage of the two techniques (TD and connectionism) can provide added power
% read http://www.stanford.edu/group/pdplab/pdphandbook/handbookch10.html

\subsubsection{Neuronal interpretation}
\paragraph{Recap:} Hebbian learning is a theoretical concept exploiting statistical correlations in passive changes of the activations of presynaptic and postsynaptic neurons. It is useful for development, i.e. the wiring of neurons for receptive fields in the brain). Reinforcement learning in contrast relies on conditional changes (reward is received for policy choosing certain actions) to maximize a reward and is therefore useful for learning new behaviours.

\paragraph{Hebbian Learning of Actions:}
example: current states as input and cues as inputs for action neurons. How to model reward on top of action neurons after they have become selective to a certain combination of states and inputs?. We only want to learn the actions that are rewarded! Hebbian Learning fails.
Most models of Hebbian Learning/STDP do not take into account neuromodulators, so cannot describe success/shock
\textsc{TODO} paragraph on Spike Timing Dependent Plasticity (STDP)

\paragraph{Neuromodulation}
Neuromodulation is the physiological process by which a given neuron uses one or more neurotransmitters to regulate diverse populations of neurons. This is in contrast to classical synaptic transmission, in which one presynaptic neuron directly influences a single postsynaptic partner. Neuromodulators secreted by a small group of neurons diffuse through large areas of the nervous system, affecting multiple neurons. Examples of neuromodulators include dopamine, serotonin, acetylcholine, histamine and others.\\
In neuroscience, an excitatory postsynaptic potential (EPSP) is a temporary depolarization of postsynaptic membrane potential caused by the flow of positively charged ions into the postsynaptic cell as a result of opening of ligand-gated ion channels. They are the opposite of inhibitory postsynaptic potentials (IPSPs), which usually result from the flow of negative ions into the cell or positive ions out of the cell. A postsynaptic potential is defined as excitatory if it makes the neuron more likely to fire an action potential. EPSPs can also result from a decrease in outgoing positive charges, while IPSPs are sometimes caused by an increase in positive charge outflow. The flow of ions that causes an EPSP is an excitatory postsynaptic current (EPSC).
EPSPs, like IPSPs, are graded (i.e. they have an additive effect). When multiple EPSPs occur on a single patch of postsynaptic membrane, their combined effect is the sum of the individual EPSPs. Larger EPSPs result in greater membrane depolarization and thus increase the likelihood that the postsynaptic cell reaches the threshold for firing an action potential.\\

extend theory
should be good for memory formation as well as \texttt{action learning}. Therefore global factors have to be included for feedback (neuromodulator)
\[
\Delta w_{ij} \propto F(\underbrace{v_i,v_j}_{local},\underbrace{\text{SUCCESS}}_{\text{\textcolor{red}{global}}})
\]

\paragraph{Timing issues in Reinforcement learning}
success signal is delayed (spike time scale:1-10ms, reward delay: 100-5000ms) $\rightarrow$ need memory trace (eligibility trace)
implement eligibility traces as memory at synapse, same definition as in $\mathbf{TD}(\lambda)$

what is winner takes all interaction here?
correlation of reward in current state and reward obtained from action
\begin{align*}
e(j,a)(t)&=\gamma\lambda e(j,a)(t-\Delta t)+\left\{\begin{array}{ll} r_jr_a & \text{if}\quad a=a'\\ 0 & \text{else}\end{array}\right.\\
&=\gamma\lambda e(j,a)(t-\Delta t)+r_j\delta_{a,a'}
\end{align*}
if a is successful the elibility trace contains its reward 
\[
e(j,a)(t)=\gamma\lambda e(j,a)(t-\Delta t)+r_j(s_t)r_a(t)
\]

\paragraph{Plasticity Model for Reinforcement learning}
spike-based model
\[
\Delta w_{ij} \propto \text{success}\cdot(v_i v_j + \dots)
\]
in contrast to old spike-timing dependent plasticity
\[
\Delta w_{ij} \propto v_i v_j + \dots
\]
synaptic update of current action a
\begin{align*}
&\Delta w_{aj}=\eta \delta_t r_j\\
&\Delta w_{aj}=\eta \delta_t r_jr_a\quad \text{if a taken}
\end{align*}
success= reward - exp. reward $\delta_t$ (dopamine neurons), success is maximized when $delta_t$ is minimized $\rightarrow$ correct prediction

\paragraph{Molecular mechanism}
changes in synaptic connections
synaptic terminal (left), dendrite (right): calcium ions poor into the nerve terminal and these release neurotransmitters from small vesicles. Some of these neurotransmitters activate receptors on the target neuron which in turn opens channels letting sodium ions in
% http://www.hhmi.org/biointeractive/molecular-mechanism-synaptic-function
% http://www.hhmi.org/research/molecular-mechanisms-regulation-synaptic-transmission-and-plasticity-brain
\textsc{TODO}

%Another way in which the agent can modify its environment is to learn state-action values instead of just state values. We define a function Q(⋅) from state-action pairs to values such that the value of Q(st,at) is the expected return for taking action at while in state st. In neural networks, the Q function is realized by having a subset of the input units represent the current state and another subset represent the possible actions. For the sake of illustration, suppose our learning agent is a robot that is moving around a grid world to find pieces of garbage. We may have a portion of the input vector to represent sensory information that is available to the robot at its current location in the grid - this portion corresponds to the state in the Q function. Another portion would be a vector for each possible action that the robot can take. Suppose that our robot can go forward, back, left, right, and pickup a piece of trash directly in front of it. Therefore, we would have a five place action vector, one place corresponding to each of the possible actions. When we must choose an action, we simply fix the current state on the input vector and turn on each of the five action units in turn, computing the value for each. The next action is chosen from these state-action values and the environment state is modified accordingly.
TD incorporates eligibility and continuous state space. We do a function approximation using weights and instead of updating $Q(s,a)$. 
TD method with a function approximator like a neural network, we update $V_t^\pi$ by finding the output gradient with respect to the weights. This step is not captured in the previous equation $\delta_t = r_{t}+\gamma Q(s',a')-Q(s,a)$. We will discuss these implementation specific details later.
%In general, we do not work with $Q(s,a)$ but rather with the $V(s)$ values.
\[
	Q_w(s,a) = \sum_{i=1}^n w_i^a * r_i(s)
\]
\[
	\Delta w_{aj} = \eta \delta_t e_{aj}
\]
\paragraph{Eligibility and continous TD}
\begin{enumerate}
\item choose action a according to policy (greedy)
\item observe reward $r$ in state $s'$, from there choose next action a'
\item calculate TD error $\delta_t$ in SARSA
\item update eligibility trace of action a in current state $e(j,a)$
\item perform weight update $\Delta w_{ja}=\eta \delta_t e(j,a)(t)$
\item update Q-value approximation for state j $Q_w(j,a)=\sum_{s' \in S'}w_{s'}^a r_{s'}(j)$ (rewards we can get through actions a leading to states s' in s) for next episode.
\item $a \leftarrow a'$, $s \leftarrow s'$, return to 2)
\end{enumerate}
eligibity traces implement (short-term) memory
The weights grow/connection is reinforced if action a at state s is successful in obtaining a reward

\paragraph{Algorithmic application: backgammon}
\textsc{todo}
DOES SOMEBODY HAVE THE BLACKBOARD: REPRESENTATION OF STATE-VALUE Lecture 7 (Backgammon)?
%As mentioned previously, TD methods have no inherent connection to neural network architectures. TD learning solves the problem of temporal credit assignment, i.e. the problem of assigning blame for error over the sequence of predictions made by the learning agent. The simplest implementation of TD learning employs a lookup table where the value of each state or state-action pair is simply stored in a table, and those values are adjusted with training. This method is effective for tasks which have an enumerable state space. However, in many tasks, and certainly in tasks with a continuous state space, we cannot hope to enumerate all possible states, or if we can, there are too many for a practical implementation. Back propagation, as we already know, solves the problem of structural credit assignment, i.e. the problem of how to adjust the weights in the network to minimize the error. One of the largest benefits of neural networks is, of course, their ability to generalize learning across similar states. Thus, combining both TD and back propagation results in an agent that can flexibly learn to maximize reward over mulitple time steps and also learn structural similarities in input patterns that allow it to generalize its predictions over novel states. This is a powerful coupling. In this section we sketch the mathematical machinery involved in coupling TD with back propagation. Then we explore a notable application of the combined TDBP algorithm to the playing of backgammon as a case study

\subsubsection{Learning algorithm for state values V}
This error will be multiplied by a learning rate $\alpha$ and then used to update our weights. The general update rule is
\[
V_t = V_t + \alpha\left[r_{t+1}+\gamma V_{t+1}-V_t\right]
\]

\subsection{Biological Principles of Learning}
\subsubsection{Spatial Learning}
Map brain: place neuronal cells - sensitive to spatial location, Environment box: place fields which are then mapped to place cells

Hippocampal place cells, depends on visual cues, works also in the dark
\paragraph{navigation to a hidden goal (Morris Water maze)}
different starting positions, task learning depends on the hippocampus
code for position of the animal, learn action (North, East, South, West, etc.) towards goal
after many trials the network should have learnt the right actions to take from positions in the water maze (the weights for the right actions should become specific to the appropriate positions.)

\subsubsection{Unsupervised learning of place cells}
from reinforcement learning of the appropriate actions for states to the unsupervised learning of place cells in the hippocampus?
\paragraph{Goal:} in order to have the code for the position of the animal in the maze we have to learn the place cells. Derive a model that stores views of visited places. 
Robot in an environment: visual input at each time step:
\paragraph{Local view:} activation of set of 9000 gabor wavelets $L(\mathbf{p},\Phi)=\{F_k\}$
store views of visited places
single view cell (VC) stores a local view
all local views are stored in an incrementally growing view cells population.

\paragraph{Extracting direction} views are aligned by current gaze directions $\Phi_i$ and their differences $\Delta\Phi=\Phi_i-\Phi_k$

\paragraph{Extracting position} small difference between local views - spatially close positions
$F_i$ from set of filter responses
\begin{align*}
&\text{Difference:} \quad \Delta L = \abs{F_i-F(t)}\\
&\text{Similarity Measure:} \quad r_i^{VC}=\exp\left(-\frac{(\Delta L)^2}{2\sigma^2_{VC}}\right)
\end{align*}

\paragraph{Learning place cells} Population of place cells created incrementally during exploration. Activity of a place:
place cells are connected to grid cells of view cells?
$\mathbf{w}_{ij}^pc$ - weight vector for place cell i
\[
r^{pc}=\sum w_{ij}^{pc}r_j^{pre}
\]
competitive hebbian learning:
\[
\delta w_{ij}^{pc}=\eta \abs{r_i^{pc}-\phi}(r_j^{pre}-w_{ij}^{pc})
\]
\paragraph{Place cell based vs. landmark-based navigation}
Place cells: all place cells connected to all possible action cells(nucleus accumbens) (important weights $w_{ij}$ are learnt). Learnt actions result in correct movement towards goal.\\
Landmark-based navigation: Visual filters connected to action cells (dorsal striatum), so that agent turns towards landmark

%The nucleus accumbens (NAcc), also known as the accumbens nucleus or as the nucleus accumbens septi (Latin for nucleus adjacent to the septum) is a region of the human brain in the basal forebrain rostral to the preoptic area.[1] The nucleus accumbens and the olfactory tubercle collectively form the ventral striatum, which is part of the basal ganglia.\\ Research has indicated the nucleus accumbens has an important role in pleasure including laughter, reward, and reinforcement learning, as well as fear, aggression, impulsivity, addiction, and the placebo effect. Each brain hemisphere has its own nucleus accumbens. It is located where the head of the caudate and the anterior portion of the putamen meet just lateral to the septum pellucidum. The nucleus accumbens can be divided into two structures—the nucleus accumbens core and the nucleus accumbens shell. These structures have different morphology and function.

%The striatum, also known as the neostriatum or striate nucleus, is a subcortical (i.e., inside, rather than on the outside) part of the forebrain. It is the major input station of the basal ganglia system. The striatum, in turn, gets input from the cerebral cortex. In primates (including humans), the striatum is divided by a white matter tract called the internal capsule into two sectors called the caudate nucleus and putamen. The term corpus striatum occasionally refers to the striatum combined with the globus pallidus, a structure closely related to the putamen, and the lenticular nucleus refers to the putamen together with the globus pallidus.\\ Functionally, the striatum helps coordinate motivation with body movement. It facilitates and balances motivation with both higher-level and lower-level functions, such as inhibiting one's behavior in a complex social interaction and fine-motor functions of inhibiting small voluntary movement.

\begin{itemize}
\item Unsupervised learning leads to interaction between vision and path integration
\item Model of landmark-based navigation
\item Model of map-based navigation
\item Learning of actions: \texttt{reinforcement learning}
\item Learning of views and places: \texttt{unsupervised learning}
\end{itemize}

\paragraph{Biological mechanisms: grid cells}
firing grids extend over the whole space. Subpopulation with different grid sizes exist. Grid cells project to the CA1 area. Grid-cell firing fields rotate following the rotation of the cue.
Biological locus of the path integration (elecrode in dorsomedial entorhinal cortex)

\subsection{Policy Gradient Methods}
Policy gradient methods are a type of reinforcement learning techniques that rely upon optimizing \texttt{parametrized policies} $\pi_\theta$ with respect to the \texttt{expected return} (long-term cumulative reward) by gradient descent. % source: scholarpedia

\subsubsection{Limitations of TD methods (Q-learning \& SARSA)}
Reinforcement learning is probably the most general framework in which reward-related learning problems of animals, humans or machine can be phrased. However, most of the methods proposed in the reinforcement learning community are not yet applicable to many problems such as robotics, motor control, etc. due to the following reasons.
The motivation for policy gradient methods is that they do not suffer from many of the problems that have been marring traditional reinforcement learning approaches:
\begin{itemize}
\item \textbf{Lack of guarantees of a value function?:} In continuous observation domains (environment) function approximations which are hard to select/tune are necessary for these methods
\item \textbf{Intractability:} In only partially observable, \texttt{non-markovian} settings, TD algorithms are shown to be fundamentally flawed. Because of the uncertain state information the learning systems need to be modeled as partially observable Markov decision problems which often results in excessive computational cost. 
\item \textbf{Convergence:} Even in fully observable, \texttt{markovian} settings, TD algorithms, e.g. Q-learning, most traditional reinforcement learning methods using function approximators have no convergence guarantees and there exist even divergence examples. 
\item \textbf{Complexity:} Continuous states and actions in high dimensional spaces cannot be treated by most reinforcement learning approaches using TD methods, because they are difficult to represent using TD methods.
\end{itemize}
Policy gradient methods differ significantly as they do not suffer from these problems in the same way. For example, uncertainty in the state might degrade the performance of the policy (if no additional state estimator is being used) but the optimization techniques for the policy do not need to be changed. Continuous states and actions can be dealt with in exactly the same way as discrete ones while, in addition, the learning performance is often increased. \texttt{Convergence} at least to a local optimum is \texttt{guaranteed}.

\subsubsection{Mathematical Formulation}
\textsc{Understand:} by associating certain actions with certain stimuli in a stochastic fashion???
\paragraph{Goal:} In policy optimization the goal is to optimize the expected reward/reward directly \[
J\left(\mathbf{\theta}\right)=E\left[\sum\nolimits_{k=0}^{H}a_{k}r_{k}\right]
\]
according to the parameters $\theta \in \mathbb{R}^K$ of the policy $\pi_\theta$, where $a_k$ denote time-step dependent weighting factors, often set to $a_k=\gamma y_k$ for discounted reinforcement learning (where $\gamma$ is in [0,1]) or $a_k=\frac{1}{H}$ for the average reward case.

\paragraph{Basic approach:}
We model the learning system for a policy gradient method in a discrete-time manner and we will denote the current time step by k. The input to the system is described using a probability distribution $s_{k+1}\sim (s_{k+1}|s_k,a_k)$ as model where $s_k \in \mathbb{R}^M$ denotes the current action, and $s_k$, $s_{k+1}\in \mathbb{R}^N$ denote the current and next state, respectively.
We furthermore assume that actions are generated by a policy $a_k \sim \pi_\theta(a_k\|s_k)$ which is modeled as a probability distribution in order to incorporate exploratory actions. The policy is assumed to be parameterized by K policy parameters $\theta \in \mathbb{R}^K$ 
The sequence of states and actions forms a \texttt{trajectory} denoted by $\tau=[s_0:H,a_0:H]$ where H denotes the \texttt{horizon} which can be infinite.The words trajectory, history, trial, or roll-out are used interchangeably. At each instant of time, the learning system receives a reward denoted by $r_k=r(s_k,a_k)\in \mathbb{R}$.

\paragraph{Optimization:} For real-world applications, any change to the policy parameterization has to be smooth because drastic changes can be hazardous for the actor. Furthermore useful initializations of the policy based on domain knowledge would otherwise vanish after a single update step. Therefore the policy method of choice is steepest descent on the expected return. The policy parametrization is thus updated according to the following gradient update rule
\[
\mathbf{\theta}_{h+1}=\mathbf{\theta}_{h}+\alpha_{h}\mathbf{\nabla
}_{\mathbf{\theta}}J\vert _{\mathbf{\theta}=\mathbf{\theta}_{h}}
\]
where $\alpha \in \mathbb{R}^+$ denotes a learning rate and $h \in \{0,1,2,\dots\}$ the current update number. The time step k and update number h are two different variables. In actor-critic-based policy gradient methods, the frequency of updates of h can be nearly as high as of k. However, in most episodic methods, the policy update h will be significantly less frequent.\\
The \texttt{main problem} in policy gradient methods is to obtain a good estimator of the policy gradient $\mathbf{\nabla}_{\mathbf{\theta}}J\vert
_{\mathbf{\theta}=\mathbf{\theta}_{h}}$

\subsubsection{Likelihood Ratio Method}
Assume that trajectories $\tau$ are generated from the policy distribution of a system $\tau \sim p\theta(\tau)=p(\tau|\theta)$ with return $r(\tau)=\sum^H_{k=0}a_k r_k$ which leads to $J(\theta)=\E\left[r(\tau)\right]=\int_T p_\theta(\tau)r(\tau)d\tau_k$. In this case, the policy gradient can be estimated using the likelihood ratio better known as REINFORCE (Williams, 1992) trick, i.e., by using
\[
\nabla_\theta p_\theta(\tau)=p_\theta(\tau)\nabla_\theta \log p_\theta(\tau)
\]
from standard differential calculus: $\mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)
=\mathbf{\nabla}_{\mathbf{\theta}}p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)/p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)$  
we obtain
\begin{align*}
\mathbf{\nabla}_{\mathbf{\theta}}J\left(\mathbf{\theta}\right)&=\int_{\mathbf{T}}\mathbf{\nabla}_{\mathbf{\theta}}p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)r(\mathbf{\tau})d\mathbf{\tau}\\
&=\int_{\mathbf{T}}p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)\mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)r(\mathbf{\tau})d\mathbf{\tau}\\
&=E\left[\mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(\mathbf{\tau}\right)r(\mathbf{\tau})\right].
\end{align*}
As the expectation $\E\left[\cdot\right]$ can be replaced by sample averages, denoted by $\langle \cdot \rangle$, only the derivative $\nabla_\theta \log p_\theta(\tau)$ is needed for the estimator. Importantly, this derivative can be computed without knowledge of the generating distribution $p_\theta(\tau)$ as
\[
p_{\mathbf{\theta}}\left(  \mathbf{\tau}\right)=p(\mathbf{s}_{0})\prod\nolimits_{k=0}
^{H}p\left(  \mathbf{s}_{k+1}\left\vert \mathbf{s}_{k},\mathbf{a}_{k}\right.
\right)  \pi_{\mathbf{\theta}}\left(  \mathbf{a}_{k}\left\vert \mathbf{s}
_{k}\right.  \right)
\]
implies that
\[
\mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(  \mathbf{\tau
}\right)  =\sum\nolimits_{k=0}^{H}\mathbf{\nabla}_{\mathbf{\theta}}\log
\pi_{\mathbf{\theta}}\left(  \mathbf{a}_{k}\left\vert \mathbf{s}_{k}\right.
\right)
\]
as only the policy depends on $\theta$.
By taking the sample average as Monte Carlo (MC) approximation of this expectation by taking N trial histories we get
\[
\mathbf{\nabla}_{\mathbf{\theta}}J=\frac{1}{N}\sum_{n=1}^N \mathbf{\nabla}_{\mathbf{\theta}}\log p_{\mathbf{\theta}}\left(\mathbf{\tau_n}\right)r(\mathbf{\tau_n}).
\]
which is a fast approximation of the policy gradient for the current policy.
\textsc{N trials or N trial histories of one trial}

\subsubsection{Drawbacks of Policy Gradient}
Of course, policy gradients are not the salvation to all problems having significant problems themselves. They are by definition on-policy (note that tricks like importance sampling can slightly alleviate this problem) and need to \texttt{forget data very fast} in order to avoid the \texttt{introduction of a bias} to the gradient estimator. \textbf{Hence, the use of sampled data is not very efficient}. In tabular representations \textbf{???}, value function methods are guaranteed to converge to a global maximum while policy gradients only converge to a local maximum and there may be many maxima in discrete problems. %Policy gradient methods are often quite demanding to apply, mainly because one has to have considerable knowledge about the system one wants to control to make reasonable policy definitions. Finally, policy gradient methods always have an open parameter, the learning rate, which may decide over the order of magnitude of the speed of convergence, these have led to new approaches inspired by expectation-maximization (see, e.g., Vlassis et al., 2009; Kober & Peters, 2008).

\subsection{Neuronal implementation of Policy Gradient Methods}
\textsc{TODO: Biology, Neurons, Hebb, Perceptron $\rightarrow$ mathing law}


\section{3 Factor rules}
\subsection{Integrate and fire model}
\begin{itemize}
\item the membrane potential v(t) defines the state of the neuron at time t
\item Each spike causes an excursion of the membrane potential with amplitude w
\item The membrane potential decays towards its resting value $V_0$
\item If v(t) reaches a threshold value $V_th$, a spike is produced and v(t) is reset to its resting value
\end{itemize}
membrane potential?
\[
C_m \frac{dv(t)}{dt}=I_{\text{leak}}(t)+I_S(t)+I_{\text{inj}}(t)
\]
decay towards resting value $V_0$
\[
I_leak(t)=-\frac{C_m}{\tau_m}\left[v(t)-V_0\right]
\]
\paragraph{Response to constant input}
sub-threshold stimulus 
\[
v(t) = V_0+I R_m(1-\exp(-(t-t_0)/\tau_m))
\]
supra-theshold stimulus
\[
T_{\text{ISI}}=-\tau_m\ln\left[1-\frac{\theta}{IR_m}\right],\quad (IR_m > \theta)
\]
where $\theta=(V_{th}-V_0)$
\paragraph{Response to spike(delta) input}
supra-threshold stimulus $I_leak(t)$ at $\Phi$ leads to post-synaptic potential with Weight J and Delay D:
\[
v_\delta(t)=J\cdot \exp(-\frac{(t-t_{sp}-D)}{\tau_m})
\]

\paragraph{Response to arbitrary input}
for any linear system, response to an arbitrary stimulus is given by the convolution of the impulse response with the stimulus
\begin{align*}
	v(t)&=V_0+\frac{1}{C_m}\int^t_{t_0}\exp(-\frac{(t-t')}{\tau_m})\cdot I_{\text{Inj}}(t')dt'\\
	&=V_0+\frac{1}{C_m}(v_\delta * I_{\text{Inj}})(t)
\end{align*}

Linear system means:
\begin{itemize}
\item if v(t) is a solution, then $\alpha v(t)$ is also a solution, for all $\alpha \in \mathbb{R}$
\item If u(t) and v(t) are solutions, then u(t)+v(t) is also a solution
\end{itemize}

\subsection{Spike response model with stochastic firing}
\paragraph{membrane potential} $\mathbf{u}$ is potential
\[
	u_i(t\vert \text{input}) = u_{rest}+\sum_{t_j^f}\varepsilon(t-t_j^f)+\sum_{t_j^f}\eta(t-t_j^f\eta(t-t_j^f)
\]
$t_j^f$ - time t wich presynaptic neuron fires

\paragraph{probabilistic spike generation}
the higher the potential (of the postsynaptic neuron) the more likely the neuron fires
\[
p(t\vert u_i)=g(u(t)) \propto \exp(\beta u(t))
\]

\paragraph{Probability of Spike Train}
$x_j(t)=\sum_j \delta(t-t_j^f)$, $\varepsilon(t-t_j^f)$, $y(t)=\sum_n\delta(t-t^n)$
\[
P(y\vert x)=\prod_n p(t\vert \hat{t}^n)\exp\left[-\int_{\hat{t}^{n-1}}^{\hat{t}^n}p(t'\vert \hat{t}^{n-1})dt'\right]
\]
potential, $\hat{t}$=refractoriness, last spike
\[
u(t) = \sum_{j,f}w_j\varepsilon\underbrace{(t-t_j^f)}_{\text{all spikes, all neurons}}+\eta(t-\hat{t})
\]
stochastic firing
\[
p(t)=g(u(t))
\]

\subsection{R-max}
\subsubsection{Derivation}
optimization of an objective function, reward maximization by gradient ascent
\begin{align*}
\frac{d}{dt}w_{ij}(t)&=\eta \frac{d}{dw_{ij}}\langle R \rangle
&=\eta \langle R(\text{spiketrain, input})\frac{d}{dw_{ij}}\log P(\text{spiketrain$\vert$ input}) \rangle
\end{align*}
Batch to online
\[
\frac{d}{dt}w_{ij}(t)\propto R(t)\int_{start}^t dt'\underbrace{\varepsilon(t'-t_j^{pre})}_{\text{\textcolor{red}{EPSP}}}\left[\underbrace{\delta(t'-t_i^f)-p(u(t'))}_{\text{Post}}\right]
\]

\subsubsection{Rule}
Simple activity model Hebb rule:
\[
\frac{d}{dt}=H_{aj}(t)=r(s_t)r_a(t)-g\cdot H_aj(t)\]
spiking hebb rule model:
\[
\frac{d}{dt}H_{aj}(t)\underbrace{\varepsilon(t'-t_j^{pre})}_{\text{\textcolor{red}{EPSP}}}\left[\underbrace{\delta(t'-t_i^f)-p(u(t'))}_{\text{Post}}\right]-g\cdot H_aj(t)
\]
\[
\frac{d}{dt}w_{aj}=\eta R_t H_{aj}
\]
$R_t$ success signal=phasic dopamine
Hebb learning rule is $\text{EPSP}\left[\delta(t-t_i^f)-p(u)\right]$\\
$\langle H_{ij} \rangle = 0$, repeating a fixed input has no Hebb-learning effect

\subsection{R-STDP}
\[
	\Delta w_{ij} \propto S(R)\times \text{STDP}
\]
\begin{align*}
\Delta w_{ij} &\propto S(R)\times f(\text{pre},\text{post})
&=S(R)\times H_{ij}
\end{align*}
Hebbian Learning Rule is STDP\\
$\langle H_{ij} \rangle \neq 0$, repeating a fixed input gives a hebb-learning effect

\subsection{When do 3-factor rules work?}
both rules discussed have the form 
\begin{align*}
\Delta w_{ij} &\propto S \cdot H_{ij}\\
&=\underbrace{\langle S\cdot H_{ij}\rangle - \langle S \rangle \langle H_{ij} \rangle}_{\text{\textcolor{green}{good: cov(R,H)}}} + \underbrace{\langle S \rangle \langle H_{ij} \rangle}_{\text{\textcolor{red}{bad}}}
\end{align*}
$\langle S \rangle \langle H_{ij} \rangle$ has to be neutralized, Always the case for R-max ($\langle H_{ij} \rangle=0$)\\
For R-STDP use $R \rightarrow S(R)=R-\langle R \rangle$ (Success=Reward-exp.Reward)\\
Good three factor rules are either 
\begin{itemize}
\item \texttt{useless} for unsupervised hebbian learning
\item or the average neuromodulatory reward signal S must be zero $S = R - \langle R \rangle \Rightarrow$ system must know running average of expected reward $\langle R \rangle$
\end{itemize}

\subsection{Application to Hand Movements}
\subsubsection{How are movements represented}
\begin{itemize}
\item muscle coordinates, vector of muscle activations over time
\item Joint coordinates - vector of joint angles over time
\item World coordinates - vector of positions in 3D space
\end{itemize}

\subsubsection{Detour: neural encoding of movement direction}
monkey pushes red button - experiment, reaching movements involve many muscles, neurons in motor cortex are tuned to movement direction, tuning curve for motor neurons, accuracy of population code

\section{Energy-Based Models (EBM)}
A scalar energy is associated to each configuration of the variables of interest in the model (cost function). Learning then corresponds to modifying the energy function so that its shape has desirable properties, for example we would like desirable configurations to have low energy. The probabilistic distribution of for the model is define by an energy function:
\[
	p(x)=\frac{\exp(-E(x))}{Z}
\]
This distribution is the Boltzmann or Gibbs distribution which originates from classical statistical mechanics.\\
The normalization factor $Z$ is called the partition function $Z=\sum_x \exp(-E(x))$.\\
An energy-based model can be learnt by performing (stochastic) gradient descent on the empirical negative log-likelihood of the training data.

\subsection{EBMs with hidden units}
Often one does not observe the examples $x$ fully or one wants to introduce some non-observed variables to increase the expressive power of the model. So we consider a hidden part $y$ additionally to the observed part $x$, giving:
\[ 
	P(x) = \sum_y P(x,y) = \sum_y \frac{\exp(-E(x,y))}{Z}
\]
By introducing the notation of \textsc{free energy}
\[
	\mathcal{F}= -\log\sum_y \exp(-\mathcal{F})
\]
we can the rewrite the probability distribution in the condensed of the gibbs distribution:
\[
	P(x)=\frac{exp(-\mathcal{F}(x))}{Z}, with Z=\sum_x \exp(-\mathcal{F}(x))
\]

\subsection{Restricted Boltzmann Machines (RBM)}
Boltzmann Machines (BMs) are a particular form of log-linear (Gibbs distribution) Markov Random Fields (MRF), where the energy function is linear in its free parameters. In order to represent complicated distributions (limited parametric setting $\rightarrow$ non-parametric setting) we consider some of the variables to be hidden (not observed). One can increase the modeling capacity of the Boltzmann Machine (BM) by adding for hidden variables/units. Furthermore a RBM is restricted to a bipartite Graph, because there no lateral connections visible-visible and hidden-hidden are allowed. We introduce the notation of $v=x$ for visible units and $h=y$ for hidden units.\\
The energy function therefore is defined as:
\[
	E(v,h)=-b^{t}v-c^{t}h-h^{t}W_{u,v}u
\]
where $W_{u,v}$ represents a connection matrix of weights for hidden and visible units and $b,c$ are the offsets of the visible and hidden layers respectively.\\
By plugging in the energy function we get the following representation of the free energy formula:\\
\[
	F(v)=-b^{t}v-\sum_i\log\sum_{h_i} \exp(h_i(c_i+W_iv))
\]

Because of the specific structure of RBMs, visible and hidden units are conditionally independent of one-another 

A particular set of ''synaptic'' weights is said to constitute a perfect model of the environmental structure if it leads to exactly the same probability distribution of the states of the visible units as when these units are clamped by the environmental input vectors.

\subsubsection*{RBMs with binary units}
Commonly RBMs are studied with binary units $v_i, h_i \in \{0,1\}$ ...
The free energy simplifies to:\\
\[
	\mathcal{F}(v)=-b^{t}v-\sum_i\log(1+\exp(c_i+W_iv))
\]

deeplearning.net/tutorial/rbm.html

sigmoid function (sigm):
\[
	\sigma(t)=sig(t)=\frac{1}{1+\exp(-t)}=\frac{1}{2}\cdot(1+\tanh(\frac{t}{2}))
\]

\end{document}
